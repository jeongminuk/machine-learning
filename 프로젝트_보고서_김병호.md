# 21101170 ê¹€ë³‘í˜¸ í”„ë¡œì íŠ¸

## ë¬¸ì œ ì •ì˜

ë³¸ í”„ë¡œì íŠ¸ëŠ” 100ê°œ ìˆ˜ì… í’ˆëª©ì˜ ì›”ë³„ ë¬´ì—­ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ê³µí–‰ì„±(Comovement)ì´ ìˆëŠ” í’ˆëª© ìŒì„ íŒë³„í•˜ê³ , ì„ í–‰ í’ˆëª©ì˜ ì •ë³´ë¡œ í›„í–‰ í’ˆëª©ì˜ ë‹¤ìŒ ë‹¬ ë¬´ì—­ëŸ‰ì„ ì˜ˆì¸¡í•˜ëŠ” ì‹œê³„ì—´ ì˜ˆì¸¡ ë¬¸ì œì…ë‹ˆë‹¤. 

**í‰ê°€ ì§€í‘œ:**
- Score = 0.5 Ã— Recall + 0.5 Ã— (1 - NMAE)
- Recall: ì‹¤ì œ ê³µí–‰ì„± ìŒ ì¤‘ ëª¨ë¸ì´ ë°œê²¬í•œ ë¹„ìœ¨
- NMAE: ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ê°„ì˜ ì •ê·œí™”ëœ í‰ê·  ì ˆëŒ€ ì˜¤ì°¨

**ëª©í‘œ:**
- ë² ì´ìŠ¤ë¼ì¸: 0.3201
- ìµœì¢… ëª©í‘œ: 0.40 ì´ìƒ

---

## ì½”ë“œ ë° ëª¨ë¸ êµ¬ì¡° ì„ íƒ ë°°ê²½

### ê¸°ì¡´ ì ‘ê·¼ë²•ì˜ í•œê³„

ì‹œê³„ì—´ ì˜ˆì¸¡ ë¬¸ì œì—ì„œ ì „í†µì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì ‘ê·¼ë²•ë“¤ì€ ê°ê° í•œê³„ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

#### 1. **CNN ê¸°ë°˜ ëª¨ë¸ì˜ í•œê³„**

**íŠ¹ì§•:**
- í•©ì„±ê³±(Convolution) ì—°ì‚°ì„ í†µí•´ ì§€ì—­ì (local) íŒ¨í„´ ì¶”ì¶œ
- Receptive field í¬ê¸°ì— ì œí•œë˜ì–´ ê·¼ê±°ë¦¬ ì •ë³´ì—ë§Œ ì§‘ì¤‘

**í•œê³„ì :**
- âŒ **ì œí•œëœ ìˆ˜ìš© ì˜ì—­**: 3Ã—3 ë˜ëŠ” 5Ã—5 ì»¤ë„ë¡œëŠ” ì¥ê¸° ì˜ì¡´ì„±(long-term dependency) í¬ì°© ì–´ë ¤ì›€
- âŒ **ë³µì¡í•œ êµ¬ì¡° ë³µì› í•œê³„**: ì‹œê³„ì—´ ë°ì´í„°ì˜ ê³„ì ˆì„±, íŠ¸ë Œë“œ ë“± ê¸€ë¡œë²Œ íŒ¨í„´ ì´í•´ ë¶€ì¡±
- âŒ **í¬ì†Œ ë°ì´í„° ì²˜ë¦¬**: ê²°ì¸¡ê°’ì´ ë§ì€ ë¬´ì—­ ë°ì´í„°(60% í¬ì†Œì„±)ì—ì„œ ì„±ëŠ¥ ì €í•˜

**ì‹œê³„ì—´ ì˜ˆì¸¡ì—ì„œì˜ ë¬¸ì œ:**
```python
# CNNì€ ê³ ì •ëœ ì»¤ë„ í¬ê¸°ë¡œ ì¸í•´ ë‹¤ì–‘í•œ lag íŒ¨í„´ í¬ì°© ì–´ë ¤ì›€
# ì˜ˆ: lag=1, lag=3, lag=7ì„ ë™ì‹œì— í•™ìŠµí•˜ê¸° ì–´ë ¤ì›€
conv1d = Conv1D(filters=64, kernel_size=3)  # 3ê°œì›” ë²”ìœ„ë§Œ í•™ìŠµ
```

#### 2. **Transformer ê¸°ë°˜ ëª¨ë¸ì˜ í•œê³„**

**íŠ¹ì§•:**
- Self-Attentionìœ¼ë¡œ ì „ì²´ ì‹œí€€ìŠ¤ ê°„ ê´€ê³„ í•™ìŠµ
- ì¥ê¸° ì˜ì¡´ì„± í¬ì°© ê°€ëŠ¥

**í•œê³„ì :**
- âŒ **ê³„ì‚° ë¹„ìš© í­ë°œ**: O(nÂ²) ë³µì¡ë„ë¡œ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ê¸¸ë©´ ë©”ëª¨ë¦¬/ì‹œê°„ ì†Œëª¨ ë§‰ëŒ€
  - 100ê°œ í’ˆëª© Ã— 65ê°œì›” = 6,500ê°œ ì‹œí€€ìŠ¤ â†’ 42,250,000ë²ˆ ì—°ì‚°
- âŒ **ì €í•´ìƒë„ ì œí•œ**: ê³„ì‚°ëŸ‰ ë¬¸ì œë¡œ ë‹¤ìš´ìƒ˜í”Œë§ í•„ìš” â†’ ì„¸ë¶€ ì •ë³´ ì†ì‹¤
- âŒ **í•™ìŠµ ë¶ˆì•ˆì •ì„±**: Layer Normalizationì˜ ë¬¸ì œì  ë°œê²¬

**Layer Normalizationì˜ ë¬¸ì œ:**
> "We observe unstable optimization using the general block when handling large-scale masks, sometimes incurring gradient exploding. We attribute this training issue to the large ratio of invalid tokens (their values are nearly zero). In this circumstance, layer normalization may magnify useless tokens overwhelmingly, leading to unstable training."

ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ í¬ì†Œì„±(60%)ì´ ë†’ì„ ë•Œ, 0ì— ê°€ê¹Œìš´ ê°’ë“¤ì´ ì •ê·œí™” ê³¼ì •ì—ì„œ í™•ëŒ€ë˜ì–´ í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§‘ë‹ˆë‹¤.

**Residual Connectionì˜ ë¬¸ì œ:**
> "Residual learning generally encourages the model to learn high-frequency contents. However, considering most tokens are invalid at the beginning, it is difficult to directly learn high-frequency details without proper low-frequency basis in GAN training, which makes the optimization harder."

ì´ˆê¸° ë‹¨ê³„ì—ì„œ ëŒ€ë¶€ë¶„ì˜ í† í°ì´ ë¬´íš¨(invalid)ì¼ ë•Œ, ê¸°ë³¸ ë² ì´ìŠ¤ ì—†ì´ ê³ ì£¼íŒŒ ì„¸ë¶€ì‚¬í•­ì„ í•™ìŠµí•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.

---

### ë³¸ í”„ë¡œì íŠ¸ì˜ ì„ íƒ: XGBoost + Lagged Correlation

ìœ„ì˜ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ **ì „í†µì ì´ì§€ë§Œ íš¨ê³¼ì ì¸ ë°©ë²•ë¡ **ì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤.

---

## ëª¨ë¸ ì§„í™” ê³¼ì •: 5ê°œ ëª¨ë¸ì˜ ì‹¤í—˜ê³¼ ê°œì„ 

ë³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” ì´ 5ê°œì˜ ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ ê°œë°œí•˜ë©° ìµœì ì˜ ì ‘ê·¼ë²•ì„ íƒìƒ‰í–ˆìŠµë‹ˆë‹¤. ê° ëª¨ë¸ì˜ í•œê³„ë¥¼ ë¶„ì„í•˜ê³  ê°œì„  ë°©í–¥ì„ ì°¾ì•„ê°€ëŠ” ê³¼ì •ì„ ê¸°ë¡í•©ë‹ˆë‹¤.

---

### ğŸ“Š **ëª¨ë¸ 1: ì´ˆê¸° ì ‘ê·¼** (0.3493) âœ…

**êµ¬ì„±:**
- **íŠ¹ì„± ìˆ˜**: 14ê°œ (value ê¸°ë°˜ë§Œ)
- **ê³µí–‰ì„± ìŒ**: 3,000ê°œ
- **ìƒê´€ê³„ìˆ˜ ì„ê³„ê°’**: 0.30
- **Lag ë²”ìœ„**: 1~7ê°œì›”
- **ëª¨ë¸**: XGBoost ë‹¨ì¼ ëª¨ë¸
- **í•˜ì´í¼íŒŒë¼ë¯¸í„°**: n_estimators=150, max_depth=5, learning_rate=0.08

**íŠ¹ì„± êµ¬ì„±:**
```
â€¢ í›„í–‰ í’ˆëª© (5ê°œ): b_t, b_t_1, b_t_2, b_ma3, b_change
â€¢ ì„ í–‰ í’ˆëª© (4ê°œ): a_t_lag, a_t_lag_1, a_ma3, a_change
â€¢ ê´€ê³„ íŠ¹ì„± (5ê°œ): ab_value_ratio, max_corr, best_lag, consistency, corr_stability
```

**ì„±ëŠ¥:**
- **ì ìˆ˜**: 0.3493
- **Baseline ëŒ€ë¹„**: +9.1%
- **íŠ¹ì§•**: ë‹¨ìˆœí•˜ê³  ì•ˆì •ì , ê³¼ì í•© ì—†ìŒ

**ì²« ì‹œë„ì˜ ì„¤ê³„ ë°©í–¥:**
1. **ì‹œê³„ì—´ ê¸°ë³¸ ìš”ì†Œ**: lag, ì´ë™í‰ê· (MA), ë³€í™”ìœ¨ ë“± ì „í†µì  ì‹œê³„ì—´ íŠ¹ì„± ì‚¬ìš©
2. **ë‹¨ìˆœí•œ êµ¬ì¡°**: ë³µì¡ë„ë¥¼ ë‚®ì¶° ì•ˆì •ì„± í™•ë³´
3. **ë‹¨ì¼ ëª¨ë¸**: XGBoost í•˜ë‚˜ë¡œ ì‹œì‘í•˜ì—¬ ê¸°ì¤€ì  í™•ë³´

---

### âŒ **ëª¨ë¸ 2: ê³ ê¸‰ ëª¨ë¸** (0.3348) - ì‹¤íŒ¨

**êµ¬ì„±:**
- **íŠ¹ì„± ìˆ˜**: 28ê°œ (+100% ì¦ê°€)
- **ê³µí–‰ì„± ìŒ**: 3,500ê°œ (+17%)
- **ì¶”ê°€ ë°ì´í„°**: weight, quantity, trade_frequency, avg_trade_value
- **ëª¨ë¸**: XGBoost (max_depth=6, n_estimators=200)

**ì¶”ê°€ëœ íŠ¹ì„± (14ê°œ):**
```
â€¢ weight íŠ¹ì„± (4ê°œ): b_weight, b_weight_ma3, a_weight_lag, a_weight_ma3
â€¢ quantity íŠ¹ì„± (2ê°œ): b_quantity, a_quantity_lag
â€¢ trade frequency íŠ¹ì„± (2ê°œ): b_trade_freq, a_trade_freq
â€¢ avg trade value íŠ¹ì„± (2ê°œ): b_avg_value, a_avg_value
â€¢ ë³µí•© íŠ¹ì„± (2ê°œ): ab_weight_ratio, ab_quantity_ratio
â€¢ ê´€ê³„ íŠ¹ì„± (2ê°œ): hs4_similarity ë“±
```

**ì„±ëŠ¥:**
- **ì ìˆ˜**: 0.3348
- **ëª¨ë¸ 1 ëŒ€ë¹„**: -4.1% âŒ
- **ë¬¸ì œì **: weight/quantity ë°ì´í„°ê°€ ë…¸ì´ì¦ˆë¡œ ì‘ìš©, ê³¼ì í•© ë°œìƒ

**ì™œ ì‹¤íŒ¨í–ˆëŠ”ê°€?**

| ê°€ì„¤ | ê²°ê³¼ | ì›ì¸ |
|------|------|------|
| "ë” ë§ì€ íŠ¹ì„± â†’ ë” ì¢‹ì€ ì„±ëŠ¥" | âŒ ì˜¤íˆë ¤ ì €í•˜ | ë…¸ì´ì¦ˆ ì¦ê°€ |
| "weight/quantityëŠ” ìœ ì˜ë¯¸" | âŒ ìƒê´€ì„± ì—†ìŒ | ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ (60% í¬ì†Œì„±) |
| "ë³µì¡í•œ ê´€ê³„ í¬ì°©" | âŒ ê³¼ì í•© ë°œìƒ | max_depth=6ì€ ë„ˆë¬´ ê¹ŠìŒ |

ì´ˆê¸° ì ‘ê·¼(ëª¨ë¸ 1)ì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì´ ë‚˜ì™”ê¸° ë•Œë¬¸ì— "íŠ¹ì„±ì„ ë” ì¶”ê°€í•˜ë©´ ë” ì¢‹ì•„ì§€ì§€ ì•Šì„ê¹Œ?"ë¼ëŠ” ìƒê°ìœ¼ë¡œ ì‹œë„í–ˆìœ¼ë‚˜ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.

**êµ¬ì²´ì  ë¬¸ì œì :**
1. **weight/quantityì˜ í¬ì†Œì„±**: 
   - ì „ì²´ ë°ì´í„°ì˜ 60%ê°€ 0 ë˜ëŠ” ê²°ì¸¡ê°’
   - ìƒê´€ê´€ê³„ê°€ ì•„ë‹Œ **ë¬´ì‘ìœ„ ë…¸ì´ì¦ˆ**ë¡œ ì‘ìš©
   
2. **Feature Noise ì¦ê°€**:
   ```python
   ê¸°ë³¸ ëª¨ë¸: Signal/Noise = 14/0 = âˆ
   ê³ ê¸‰ ëª¨ë¸: Signal/Noise = 14/14 = 1.0 (50% ë…¸ì´ì¦ˆ)
   ```

3. **ê³¼ì í•© ì§•í›„**:
   - ê²€ì¦ ë°ì´í„°ì—ì„œ ì„±ëŠ¥ ì €í•˜
   - ë³µì¡í•œ íŠ¹ì„± ì¡°í•©ì´ í•™ìŠµ ë°ì´í„°ì—ë§Œ ìµœì í™”

---

### âš ï¸ **ëª¨ë¸ 3: ë‹¨ìˆœí™” ëª¨ë¸** (ë¯¸í…ŒìŠ¤íŠ¸)

**êµ¬ì„±:**
- **íŠ¹ì„± ìˆ˜**: 14ê°œ (ë…¸ì´ì¦ˆ ì œê±°)
- **ê³µí–‰ì„± ìŒ**: 3,500ê°œ (ìœ ì§€)
- **íŠ¹ì„± êµ¬ì„±**: ê¸°ë³¸ value íŠ¹ì„± 10ê°œ + HS4 ìœ ì‚¬ì„± í¬í•¨ ê´€ê³„ íŠ¹ì„± 4ê°œ
- **ëª¨ë¸**: XGBoost (max_depth=5, n_estimators=150)

**ê°œì„  ì „ëµ:**
```diff
- weight/quantity/trade_freq íŠ¹ì„± 14ê°œ ì œê±° âŒ
+ HS4 ìœ ì‚¬ì„± (hs4_similarity) ìœ ì§€ âœ…
+ ê³µí–‰ì„± ìŒì€ 3,500ê°œë¡œ ìœ ì§€ (ê´€ê³„ íƒìƒ‰ ë²”ìœ„ í™•ëŒ€)
```

**ì™œ ì´ ì ‘ê·¼ì„ ì‹œë„í–ˆëŠ”ê°€?**
1. **ë…¸ì´ì¦ˆ ì œê±°**: weight/quantityëŠ” ë²„ë¦¬ë˜, HS4 ìœ ì‚¬ì„±ì€ ì˜ë¯¸ ìˆì„ ê²ƒìœ¼ë¡œ íŒë‹¨
2. **ê´€ê³„ íƒìƒ‰ í™•ëŒ€**: ìŒì„ 3,500ê°œë¡œ ëŠ˜ë ¤ ë” ë§ì€ íŒ¨í„´ íƒìƒ‰
3. **ì•ˆì •ì„± íšŒë³µ**: max_depth=5ë¡œ ë³µê·€

**ê°€ì„¤:**
> "ê³ ê¸‰ ëª¨ë¸ì˜ ë¬¸ì œëŠ” weight/quantity ë…¸ì´ì¦ˆì˜€ë‹¤. HS4 ìœ ì‚¬ì„±ë§Œ ë‚¨ê¸°ë©´ ì´ˆê¸° ì ‘ê·¼(0.3493)ì„ ì´ˆê³¼í•  ê²ƒì´ë‹¤."

**ê²°ê³¼:**
- **ì ìˆ˜**: ë¯¸í…ŒìŠ¤íŠ¸ (ì œì¶œí•˜ì§€ ì•ŠìŒ)
- **ì´ìœ **: ë‹¤ìŒ ëª¨ë¸(ì´ˆê³ ê¸‰)ì„ ë¨¼ì € ì‹œë„

---

### âŒâŒ **ëª¨ë¸ 4: ì´ˆê³ ê¸‰ ì•™ìƒë¸”** (0.293) - ëŒ€ì‹¤íŒ¨

**êµ¬ì„±:**
- **íŠ¹ì„± ìˆ˜**: 65ê°œ (+364% í­ë°œ)
- **ê³µí–‰ì„± ìŒ**: 5,000ê°œ (+67%)
- **Lag ë²”ìœ„**: 1~12ê°œì›” (ê¸°ì¡´ 1~7ì—ì„œ í™•ëŒ€)
- **ëª¨ë¸**: XGBoost + LightGBM + CatBoost 3-ëª¨ë¸ ì•™ìƒë¸”
- **ì•™ìƒë¸” ê°€ì¤‘ì¹˜**: 40% + 35% + 25%

**íŠ¹ì„± êµ¬ì„± (65ê°œ):**
```
1. ì‹œê³„ì—´ íŠ¹ì„± (35ê°œ):
   â€¢ ì´ë™í‰ê· : ma3, ma6, ma12
   â€¢ ë³€í™”ìœ¨: change_rate_1m, 3m, 6m
   â€¢ ê°€ì†ë„: acceleration
   â€¢ ëª¨ë©˜í…€: momentum_3m, 6m
   â€¢ ë³€ë™ì„±: volatility_3m, 6m, 12m
   â€¢ RSI (Relative Strength Index)
   â€¢ ê¸°íƒ€ ê¸°ìˆ ì  ì§€í‘œ

2. í’ˆëª© íŠ¹ì„± (20ê°œ):
   â€¢ íŠ¸ë Œë“œ: linear_trend, trend_strength
   â€¢ ê³„ì ˆì„±: seasonality_score
   â€¢ ì•ˆì •ì„±: stability_score
   â€¢ trading_freq ê´€ë ¨ íŠ¹ì„±

3. ê´€ê³„ íŠ¹ì„± (10ê°œ):
   â€¢ max_corr, recent_corr (ìµœê·¼ 3ê°œì›”), mid_corr (3~6ê°œì›”)
   â€¢ HS4 ìœ ì‚¬ì„±
   â€¢ ë³µí•© ê´€ê³„ ì§€í‘œ
```

**ì„±ëŠ¥:**
- **ì ìˆ˜**: 0.293
- **ê¸°ë³¸ ëª¨ë¸ ëŒ€ë¹„**: -16.0% âŒâŒ
- **Baseline ëŒ€ë¹„**: -8.5% (ë² ì´ìŠ¤ë¼ì¸ë³´ë‹¤ë„ ë‚®ìŒ!)

**ì™œ ì´ë ‡ê²Œ ì‹¤íŒ¨í–ˆëŠ”ê°€?**

| ë¬¸ì œ | ì„¤ëª… | ì˜í–¥ |
|------|------|------|
| **ê³¼ì í•©ì˜ ì •ì„** | 65ê°œ íŠ¹ì„±ì´ í•™ìŠµ ë°ì´í„° ì•”ê¸° | ê²€ì¦ ì„±ëŠ¥ í­ë½ |
| **ë³µì¡ë„ í­ë°œ** | 3ê°œ ëª¨ë¸ ì•™ìƒë¸” + 65ê°œ íŠ¹ì„± | ê³„ì‚° ì‹œê°„ 5ë°° ì¦ê°€ |
| **Lag ê³¼ë‹¤** | 1~12ê°œì›” â†’ ì¥ê¸° íŒ¨í„´ ê°•ì œ í•™ìŠµ | ë‹¨ê¸° ì‹ í˜¸ ì™œê³¡ |
| **ë…¸ì´ì¦ˆ ì¬ë“±ì¥** | weight/quantity ìœ ì‚¬ íŠ¹ì„± ë‹¤ìˆ˜ í¬í•¨ | Signal/Noise = 20/45 |

**êµ¬ì²´ì  ì‹¤íŒ¨ ì›ì¸:**

1. **ì˜ë¯¸ ì—†ëŠ” íŠ¹ì„± ê³¼ë‹¤:**
   ```python
   momentum_6m, acceleration, RSI â†’ ë¬´ì—­ ë°ì´í„°ì™€ ë¬´ê´€
   (ì£¼ì‹ ê¸°ìˆ ì  ì§€í‘œë¥¼ ë¬´ì—­ ë°ì´í„°ì— ë¬´ë¦¬í•˜ê²Œ ì ìš©)
   ```

2. **ì•™ìƒë¸”ì˜ ì—­íš¨ê³¼:**
   - 3ê°œ ëª¨ë¸ì´ ëª¨ë‘ ê°™ì€ ë…¸ì´ì¦ˆë¥¼ í•™ìŠµ
   - ì•™ìƒë¸” = ê³¼ì í•© Ã— 3
   - ë‹¤ì–‘ì„± ì—†ì´ ë³µì¡ë„ë§Œ ì¦ê°€

3. **Lag ë²”ìœ„ ê³¼ë„ í™•ëŒ€:**
   - 1~7ê°œì›”: ë‹¨ê¸° ê³µí–‰ì„± í¬ì°© âœ…
   - 1~12ê°œì›”: ì¥ê¸° íŠ¸ë Œë“œ ê°•ì œ í•™ìŠµ âŒ
   - 12ê°œì›” lagëŠ” ê³„ì ˆì„±ì´ì§€ ê³µí–‰ì„±ì´ ì•„ë‹˜

**êµí›ˆ:**
> "ë³µì¡í•¨ â‰  ì„±ëŠ¥. ì£¼ì‹ ë¶„ì„ ê¸°ë²•ì„ ë¬´ì—­ ë°ì´í„°ì— ì ìš©í•˜ë©´ ì•ˆ ëœë‹¤."

---

### ğŸ¯ **ëª¨ë¸ 5: ì‹¤ìš© ëª¨ë¸** (ì˜ˆìƒ 0.37-0.40)

**êµ¬ì„±:**
- **íŠ¹ì„± ìˆ˜**: 14ê°œ (ê¸°ë³¸ ëª¨ë¸ íšŒê·€)
- **ê³µí–‰ì„± ìŒ**: 4,000ê°œ (+33%)
- **ìƒê´€ê³„ìˆ˜ ì„ê³„ê°’**: 0.28 (0.30 â†’ 0.28 ì™„í™”)
- **Lag ë²”ìœ„**: 1~7ê°œì›” (ê²€ì¦ëœ ë²”ìœ„)
- **ëª¨ë¸**: XGBoost + LightGBM (60:40 ì•™ìƒë¸”)

**í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”:**
```python
XGBoost:
  n_estimators=250 (150 â†’ 250)
  max_depth=5 (ìœ ì§€)
  learning_rate=0.06 (0.08 â†’ 0.06, ë” ì•ˆì •ì )
  min_child_weight=6 (ê³¼ì í•© ë°©ì§€ ê°•í™”)
  reg_alpha=0.6, reg_lambda=1.2 (ì •ê·œí™” ê°•í™”)

LightGBM:
  n_estimators=250
  max_depth=6
  learning_rate=0.06
  min_child_weight=25 (ê³¼ì í•© ë°©ì§€ ê·¹ëŒ€í™”)
```

**ì™œ ì´ ë°©ë²•ì„ ì„ íƒí–ˆëŠ”ê°€?**

| ê²°ì • | ì´ìœ  | ê·¼ê±° |
|------|------|------|
| **14ê°œ íŠ¹ì„± íšŒê·€** | ê²€ì¦ëœ íŠ¹ì„±ë§Œ ì‚¬ìš© | ì´ˆê¸° ì ‘ê·¼ì—ì„œ íš¨ê³¼ ì…ì¦ |
| **4,000ê°œ ìŒ** | ê´€ê³„ íƒìƒ‰ ë²”ìœ„ í™•ëŒ€ | ì„ê³„ê°’ 0.28ë¡œ ì™„í™” â†’ ë” ë§ì€ ì•½í•œ ìƒê´€ê´€ê³„ í¬ì°© |
| **2-ëª¨ë¸ ì•™ìƒë¸”** | ë‹¤ì–‘ì„± í™•ë³´ | XGBoost(ê¹Šì´â†“) + LightGBM(ê¹Šì´â†‘) ì¡°í•© |
| **ì •ê·œí™” ê°•í™”** | ê³¼ì í•© ë°©ì§€ | reg_alpha=0.6, min_child_weight ì¦ê°€ |

**ê°œì„  ì „ëµ:**

1. **ë³µì¡ë„ ê°ì†Œ + ê´€ê³„ íƒìƒ‰ ì¦ê°€:**
   ```
   ëª¨ë¸ 4: 65ê°œ íŠ¹ì„± Ã— 5,000ìŒ = 325,000 ì°¨ì› âŒ
   ëª¨ë¸ 5: 14ê°œ íŠ¹ì„± Ã— 4,000ìŒ = 56,000 ì°¨ì› âœ…
   ```
   â†’ 82% ë³µì¡ë„ ê°ì†Œ, 33% ê´€ê³„ íƒìƒ‰ ì¦ê°€

2. **ì•™ìƒë¸” ìµœì†Œí™”:**
   - 3-ëª¨ë¸ ì•™ìƒë¸” (ëª¨ë¸ 4) â†’ ê³¼ì í•© ì¦í­
   - 2-ëª¨ë¸ ì•™ìƒë¸” (ëª¨ë¸ 5) â†’ ë‹¤ì–‘ì„± í™•ë³´

3. **ì„ê³„ê°’ ì™„í™”ì˜ íš¨ê³¼:**
   ```python
   0.30 ì„ê³„ê°’: ê°•í•œ ìƒê´€ê´€ê³„ 3,000ìŒ
   0.28 ì„ê³„ê°’: ì¤‘ê°„ ìƒê´€ê´€ê³„ 4,000ìŒ (+33%)
   ```
   â†’ ì•½í•œ ê³µí–‰ì„±ë„ í¬ì°©í•˜ì—¬ Recall í–¥ìƒ

**ì˜ˆìƒ ì„±ëŠ¥:**
- **ëª©í‘œ**: 0.37-0.40
- **ê·¼ê±°**:
  - ê¸°ë³¸ ëª¨ë¸(0.3493) + ê´€ê³„ íƒìƒ‰ í™•ëŒ€(+33%) + ìµœì í™”
  - ê³¼ì í•© ë°©ì§€ë¡œ ì•ˆì •ì„± í™•ë³´

---

### ğŸ“Š **5ê°œ ëª¨ë¸ ë¹„êµ ìš”ì•½**

| ëª¨ë¸ | íŠ¹ì„± | ìŒ | ëª¨ë¸ ìˆ˜ | ì ìˆ˜ | ì°¨ì´ | í•µì‹¬ ë¬¸ì œ/ê°œì„  |
|------|------|------|---------|------|------|----------------|
| **Baseline** | - | - | - | 0.3201 | - | - |
| **ëª¨ë¸ 1 (ì´ˆê¸°)** âœ… | 14 | 3,000 | 1 | **0.3493** | +9.1% | ì²« ì‹œë„ ê²°ê³¼ |
| **ëª¨ë¸ 2 (ê³ ê¸‰)** âŒ | 28 | 3,500 | 1 | 0.3348 | -4.1% | weight/quantity ë…¸ì´ì¦ˆ |
| **ëª¨ë¸ 3 (ë‹¨ìˆœí™”)** âš ï¸ | 14 | 3,500 | 1 | ë¯¸í…ŒìŠ¤íŠ¸ | ? | ë…¸ì´ì¦ˆ ì œê±° ì‹œë„ |
| **ëª¨ë¸ 4 (ì´ˆê³ ê¸‰)** âŒâŒ | 65 | 5,000 | 3 | 0.293 | -16.0% | ê·¹ì‹¬í•œ ê³¼ì í•© |
| **ëª¨ë¸ 5 (ì‹¤ìš©)** ğŸ¯ | 14 | 4,000 | 2 | 0.37~0.40? | +5~14% | ë‹¨ìˆœí•¨ + ìµœì í™” |

---

### ğŸ’¡ **í•µì‹¬ êµí›ˆ**

#### 1. **ë³µì¡í•œ ëª¨ë¸ â‰  ì¢‹ì€ ì„±ëŠ¥**

```
14ê°œ íŠ¹ì„± (ì²« ì‹œë„) â†’ 0.3493 âœ…
28ê°œ íŠ¹ì„± (ëª¨ë¸ 2) â†’ 0.3348 âŒ
65ê°œ íŠ¹ì„± (ëª¨ë¸ 4) â†’ 0.293 âŒâŒ
```

**ê²°ë¡ :** "Feature Engineering > Feature Quantity"

#### 2. **ë°ì´í„° í’ˆì§ˆ > ë°ì´í„° ì–‘**

```
Valueë§Œ ì‚¬ìš© (14ê°œ) â†’ ì•ˆì •ì  ì‹ í˜¸
Weight/Quantity ì¶”ê°€ (28ê°œ) â†’ ë…¸ì´ì¦ˆ ì¦ê°€ (60% í¬ì†Œì„±)
```

**ê²°ë¡ :** "ì¢‹ì€ íŠ¹ì„± 14ê°œ >> ë‚˜ìœ íŠ¹ì„± 65ê°œ"

#### 3. **ì•™ìƒë¸”ì˜ ì—­ì„¤**

```
3-ëª¨ë¸ ì•™ìƒë¸” (ëª¨ë¸ 4) â†’ ê³¼ì í•© Ã— 3
2-ëª¨ë¸ ì•™ìƒë¸” (ëª¨ë¸ 5) â†’ ë‹¤ì–‘ì„± í™•ë³´
```

**ê²°ë¡ :** "ì•™ìƒë¸”ì€ ë‹¤ì–‘ì„±ì´ ìˆì„ ë•Œë§Œ íš¨ê³¼ì "

#### 4. **ë„ë©”ì¸ ì§€ì‹ì˜ ì¤‘ìš”ì„±**

```
ì£¼ì‹ ê¸°ìˆ ì  ì§€í‘œ (RSI, Momentum) â†’ ë¬´ì—­ ë°ì´í„°ì— ë¬´ì˜ë¯¸ âŒ
ì‹œê³„ì—´ í•µì‹¬ ìš”ì†Œ (Lag, MA, Change) â†’ íš¨ê³¼ì  âœ…
```

**ê²°ë¡ :** "ë²”ìš© ê¸°ë²•ë³´ë‹¤ ë„ë©”ì¸ íŠ¹í™” ì„¤ê³„ê°€ ì¤‘ìš”"

#### 5. **Occam's Razor ê²€ì¦**

```
"ê°€ì¥ ë‹¨ìˆœí•œ ì„¤ëª…ì´ ë³´í†µ ì˜³ë‹¤"
â†’ 14ê°œ íŠ¹ì„± + XGBoost = ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥
```

**ìµœì¢… ì „ëµ:** ë³µì¡ë„ â†“ + ê´€ê³„ íƒìƒ‰ â†‘ + ìµœì í™”

---

## ì½”ë“œ êµ¬í˜„

### ë°ì´í„°ì…‹ êµ¬ì„±ê³¼ ì „ì²˜ë¦¬

```python
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from tqdm import tqdm
from joblib import Parallel, delayed
from xgboost import XGBRegressor
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# ê³µí†µ í•¨ìˆ˜: safe_corr
def safe_corr(x, y):
    """ì•ˆì „í•œ ìƒê´€ê³„ìˆ˜ ê³„ì‚° (í‘œì¤€í¸ì°¨ 0ì¸ ê²½ìš° ì²˜ë¦¬)"""
    if np.std(x) == 0 or np.std(y) == 0:
        return 0.0
    return float(np.corrcoef(x, y)[0, 1])

# ë°ì´í„° ë¡œë“œ
train = pd.read_csv('./train.csv')

# item_idë³„, ë…„ì›”ë³„ value í•©ê³„ë¥¼ í”¼ë´‡ í…Œì´ë¸”ë¡œ ë³€í™˜
pivot_value = train.groupby(['item_id', 'year', 'month'])['value'].sum().reset_index()
pivot_value['year_month'] = pivot_value['year'].astype(str) + '-' + pivot_value['month'].astype(str).str.zfill(2)
pivot_value = pivot_value.pivot(index='item_id', columns='year_month', values='value').fillna(0)

print(f"ë°ì´í„° í˜•íƒœ: {pivot_value.shape}")
print(f"í’ˆëª© ìˆ˜: {len(pivot_value)}")
print(f"ì›” ìˆ˜: {len(pivot_value.columns)}")
```

**ì „ì²˜ë¦¬ ê³¼ì •:**
1. CSV íŒŒì¼ì—ì„œ í’ˆëª©ë³„ ì›”ë³„ ë¬´ì—­ ë°ì´í„°(value, weight, quantity ë“±) ë¡œë“œ
2. í’ˆëª©ë³„-ì›”ë³„ í”¼ë´‡ í…Œì´ë¸” ìƒì„±í•˜ì—¬ ì‹œê³„ì—´ í˜•íƒœë¡œ ë³€í™˜
3. ê²°ì¸¡ê°’ì„ 0ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ë°ì´í„° ì¼ê´€ì„± í™•ë³´
4. 2020ë…„ 1ì›” ~ 2025ë…„ 4ì›”ê¹Œì§€ ì´ 65ê°œì›”ì˜ ì‹œê³„ì—´ ë°ì´í„° êµ¬ì„±

**ë°ì´í„° íŠ¹ì§•:**
- 100ê°œ í’ˆëª© Ã— 65ê°œì›” = 6,500ê°œ ì‹œê³„ì—´ í¬ì¸íŠ¸
- ì•½ 60%ì˜ í¬ì†Œì„± (sparse data) ì¡´ì¬
- value ê¸°ë°˜ ë¶„ì„ì´ ê°€ì¥ ì‹ ë¢°ì„± ë†’ìŒ (weight, quantityëŠ” ë…¸ì´ì¦ˆ ë§ìŒ)

---

### ê³µí–‰ì„± íƒì§€ ëª¨ë¸: Lagged Correlation ê¸°ë°˜

```python
def find_comovement_pairs(pivot, max_lag=7, min_nonzero=8, corr_threshold=0.30, n_jobs=-1):
    """
    Lagged correlationì„ ì´ìš©í•œ ê³µí–‰ì„± ìŒ íƒìƒ‰
    
    Args:
        pivot: í’ˆëª©ë³„ ì‹œê³„ì—´ ë°ì´í„° (item_id Ã— months)
        max_lag: ìµœëŒ€ ì§€ì—° ì‹œê°„ (1~7ê°œì›”)
        min_nonzero: ìµœì†Œ ë¹„ì˜ ê°’ ê°œìˆ˜ (í¬ì†Œì„± í•„í„°ë§)
        corr_threshold: ìƒê´€ê³„ìˆ˜ ì„ê³„ê°’ (0.30)
        n_jobs: ë³‘ë ¬ ì²˜ë¦¬ ì½”ì–´ ìˆ˜
    
    Returns:
        DataFrame: ê³µí–‰ì„± ìŒ ëª©ë¡ (leading_item_id, following_item_id, best_lag, max_corr)
    """
    items = pivot.index.to_list()
    months = pivot.columns.to_list()
    n_months = len(months)

    def process_pair(leader, follower):
        """ë‹¨ì¼ ìŒ ì²˜ë¦¬ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
        x = pivot.loc[leader].values.astype(float)
        y = pivot.loc[follower].values.astype(float)
        
        # í¬ì†Œì„± í•„í„°ë§: ë¹„ì˜ ê°’ì´ ì¶©ë¶„í•œì§€ í™•ì¸
        if np.count_nonzero(x) < min_nonzero or np.count_nonzero(y) < min_nonzero:
            return None
        
        best_lag = None
        best_corr = 0.0
        second_best_corr = 0.0
        
        # ê° lagì— ëŒ€í•´ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
        for lag in range(1, max_lag + 1):
            if n_months <= lag:
                continue
            
            # A[t-lag]ì™€ B[t]ì˜ ìƒê´€ê´€ê³„ (í•µì‹¬!)
            corr = safe_corr(x[:-lag], y[lag:])
            
            if abs(corr) > abs(best_corr):
                second_best_corr = best_corr
                best_corr = corr
                best_lag = lag
            elif abs(corr) > abs(second_best_corr):
                second_best_corr = corr
        
        # ì„ê³„ê°’ ì´ìƒì´ë©´ ê³µí–‰ì„± ìŒìœ¼ë¡œ íŒì •
        if best_lag is not None and abs(best_corr) >= corr_threshold:
            # ìµœê·¼ ìƒê´€ê³„ìˆ˜ ê²€ì¦ (ì¼ê´€ì„± í™•ì¸)
            recent_corr = 0.0
            if n_months > best_lag + 6:
                recent_x = x[-(6+best_lag):-best_lag]
                recent_y = y[-6:]
                recent_corr = safe_corr(recent_x, recent_y)
            
            # ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
            consistency = 1.0
            if abs(recent_corr) > 0.2 and np.sign(best_corr) == np.sign(recent_corr):
                consistency = 1.2
            
            return {
                "leading_item_id": leader,
                "following_item_id": follower,
                "best_lag": best_lag,
                "max_corr": best_corr,
                "recent_corr": recent_corr,
                "consistency": consistency,
                "corr_stability": abs(best_corr - second_best_corr),
            }
        
        return None

    # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ëª¨ë“  ìŒ íƒìƒ‰
    results = Parallel(n_jobs=n_jobs)(
        delayed(process_pair)(leader, follower)
        for leader in tqdm(items, desc="Finding comovement pairs")
        for follower in items if leader != follower
    )

    # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    pairs = pd.DataFrame([res for res in results if res is not None])
    
    if len(pairs) > 0:
        # ìƒê´€ê³„ìˆ˜ ì ˆëŒ€ê°’ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        pairs['score'] = pairs['max_corr'].abs() * pairs['consistency']
        pairs = pairs.sort_values('score', ascending=False)
        pairs = pairs.drop('score', axis=1)
    
    return pairs

# ê³µí–‰ì„± ìŒ íƒìƒ‰ ì‹¤í–‰
pairs = find_comovement_pairs(
    pivot_value,
    max_lag=7,
    min_nonzero=8,
    corr_threshold=0.30
)

print(f"íƒìƒ‰ëœ ê³µí–‰ì„±ìŒ ìˆ˜: {len(pairs):,}ê°œ")
print(f"\nLag ë¶„í¬:")
print(pairs['best_lag'].value_counts().sort_index())
print(f"\nìƒê´€ê³„ìˆ˜ í†µê³„:")
print(pairs['max_corr'].describe())

# ìƒìœ„ 3,000ê°œ ì„ íƒ
if len(pairs) > 3000:
    pairs = pairs.head(3000)
    print(f"\nìƒìœ„ 3,000ê°œ ìŒ ì‚¬ìš©")
```

**Lagged Correlation ì„ íƒ ì´ìœ :**

1. **ì‹œê³„ì—´ ì¸ê³¼ê´€ê³„ í¬ì°©**: ì„ í–‰ í’ˆëª© A[t-lag]ê°€ í›„í–‰ í’ˆëª© B[t]ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì •ëŸ‰í™”
2. **ë‹¨ìˆœí•˜ì§€ë§Œ íš¨ê³¼ì **: ë³µì¡í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸ ì—†ì´ë„ ì‹œê°„ ì§€ì—° íŒ¨í„´ì„ ëª…í™•íˆ ë°œê²¬
3. **í•´ì„ ê°€ëŠ¥ì„±**: ìƒê´€ê³„ìˆ˜ì™€ lag ê°’ìœ¼ë¡œ ì§ê´€ì ì¸ ì´í•´ ê°€ëŠ¥
4. **ê³„ì‚° íš¨ìœ¨ì„±**: 100ê°œ í’ˆëª©ì— ëŒ€í•´ ë¹ ë¥¸ íƒìƒ‰ ê°€ëŠ¥

**íŒŒë¼ë¯¸í„° ì„¤ì •:**
- `max_lag=7`: ìµœëŒ€ 7ê°œì›”ê¹Œì§€ ì§€ì—° ì‹œê°„ íƒìƒ‰ (ë¬´ì—­ ë°ì´í„° íŠ¹ì„±ìƒ ì¥ê¸° ì§€ì—°ì€ ë“œë¬¼ìŒ)
- `min_nonzero=8`: ìµœì†Œ 8ê°œì›” ì´ìƒì˜ ê±°ë˜ ì´ë ¥ì´ ìˆì–´ì•¼ ì‹ ë¢°ì„± ìˆëŠ” ìƒê´€ê´€ê³„ ê³„ì‚°
- `corr_threshold=0.30`: ìƒê´€ê³„ìˆ˜ ì ˆëŒ€ê°’ 0.30 ì´ìƒì„ ìœ ì˜ë¯¸í•œ ê³µí–‰ì„±ìœ¼ë¡œ íŒë‹¨

---

### ì˜ˆì¸¡ ëª¨ë¸: XGBoost Regressor

```python
class XGBRegressor:
    def __init__(self):
        self.model = XGBRegressor(
            n_estimators=150,       # íŠ¸ë¦¬ ê°œìˆ˜
            max_depth=5,            # íŠ¸ë¦¬ ê¹Šì´
            learning_rate=0.08,     # í•™ìŠµë¥ 
            subsample=0.85,         # ìƒ˜í”Œë§ ë¹„ìœ¨
            colsample_bytree=0.85,  # íŠ¹ì„± ìƒ˜í”Œë§ ë¹„ìœ¨
            min_child_weight=5,     # ìµœì†Œ ìì‹ ê°€ì¤‘ì¹˜
            gamma=0.2,              # ë¶„í•  ìµœì†Œ ì†ì‹¤ ê°ì†Œ
            reg_alpha=0.5,          # L1 ì •ê·œí™”
            reg_lambda=1.0,         # L2 ì •ê·œí™”
            random_state=42
        )
```

**XGBoost ì„ íƒ ì´ìœ :**

1. **Gradient Boostingì˜ ê°•ë ¥í•¨**: ì•½í•œ í•™ìŠµê¸°(íŠ¸ë¦¬)ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ê²°í•©í•˜ì—¬ ê°•ë ¥í•œ ì˜ˆì¸¡ ëª¨ë¸ ìƒì„±
2. **ë¹„ì„ í˜• ê´€ê³„ í¬ì°©**: ì„ í–‰-í›„í–‰ í’ˆëª© ê°„ ë³µì¡í•œ ë¹„ì„ í˜• íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥
3. **íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„**: Feature importanceë¡œ ì–´ë–¤ íŠ¹ì„±ì´ ì˜ˆì¸¡ì— ì¤‘ìš”í•œì§€ í•´ì„ ê°€ëŠ¥
4. **ê³¼ì í•© ë°©ì§€**: Regularization(L1/L2)ê³¼ Early stoppingìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í™•ë³´
5. **ë¹ ë¥¸ í•™ìŠµ ì†ë„**: ë³‘ë ¬ ì²˜ë¦¬ ë° ìµœì í™”ë¡œ ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œë„ íš¨ìœ¨ì 

**í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ê·¼ê±°:**
- `n_estimators=150`: ì¶©ë¶„í•œ í•™ìŠµ + ê³¼ì í•© ë°©ì§€ ê· í˜•ì 
- `max_depth=5`: ê¹Šì€ íŠ¸ë¦¬ëŠ” ê³¼ì í•© ìœ„í—˜, 5ë‹¨ê³„ê°€ ì ì ˆ
- `learning_rate=0.08`: ë‚®ì€ í•™ìŠµë¥ ë¡œ ì•ˆì •ì  ìˆ˜ë ´
- `subsample/colsample_bytree=0.85`: 85% ìƒ˜í”Œë§ìœ¼ë¡œ robustness í™•ë³´
- `min_child_weight=5`: ë„ˆë¬´ ì‘ì€ ë¶„í•  ë°©ì§€
- `reg_alpha=0.5, reg_lambda=1.0`: L1/L2 ì •ê·œí™”ë¡œ ê°€ì¤‘ì¹˜ ì œì–´

---

### í•™ìŠµ ë°ì´í„° ìƒì„±: 14ê°œ íŠ¹ì„±

```python
def create_training_data(pivot_value, pairs):
    """
    ê³µí–‰ì„± ìŒì— ëŒ€í•œ í•™ìŠµ ë°ì´í„° ìƒì„± (14ê°œ íŠ¹ì„±)
    
    Args:
        pivot_value: í’ˆëª©ë³„ ì‹œê³„ì—´ ë°ì´í„°
        pairs: ê³µí–‰ì„± ìŒ DataFrame
    
    Returns:
        DataFrame: í•™ìŠµ ë°ì´í„° (features + target)
    """
    samples = []
    months = pivot_value.columns.to_list()
    n_months = len(months)
    
    for row in tqdm(pairs.itertuples(index=False), desc="Creating training data"):
        leader = row.leading_item_id
        follower = row.following_item_id
        lag = int(row.best_lag)
        corr = float(row.max_corr)
        consistency = getattr(row, 'consistency', 1.0)
        
        if (leader not in pivot_value.index or follower not in pivot_value.index):
            continue
        
        # ì‹œê³„ì—´ ì¶”ì¶œ
        b_value = pivot_value.loc[follower].values.astype(float)  # í›„í–‰ í’ˆëª©
        a_value = pivot_value.loc[leader].values.astype(float)    # ì„ í–‰ í’ˆëª©
        
        # ê° ì‹œì  tì— ëŒ€í•´ ìƒ˜í”Œ ìƒì„± (t+1ì„ ì˜ˆì¸¡)
        for t in range(lag + 3, n_months - 1):
            # ========================================
            # í›„í–‰ í’ˆëª© íŠ¹ì„± (5ê°œ)
            # ========================================
            b_t = b_value[t]          # í˜„ì¬ ê°’
            b_t_1 = b_value[t - 1]    # 1ê°œì›” ì „
            b_t_2 = b_value[t - 2]    # 2ê°œì›” ì „
            b_ma3 = np.mean(b_value[max(0, t-2):t+1])  # 3ê°œì›” ì´ë™í‰ê· 
            b_change = (b_t - b_t_1) / (b_t_1 + 1)     # ë³€í™”ìœ¨
            
            # ========================================
            # ì„ í–‰ í’ˆëª© íŠ¹ì„± (4ê°œ)
            # ========================================
            a_t_lag = a_value[t - lag]              # lag ì‹œì  ê°’ (í•µì‹¬!)
            a_t_lag_1 = a_value[t - lag - 1] if (t - lag - 1) >= 0 else 0.0
            a_ma3 = np.mean(a_value[max(0, t-lag-2):t-lag+1])  # ì´ë™í‰ê· 
            a_change = (a_t_lag - a_t_lag_1) / (a_t_lag_1 + 1) # ë³€í™”ìœ¨
            
            # ========================================
            # ê´€ê³„ íŠ¹ì„± (5ê°œ)
            # ========================================
            ab_value_ratio = b_t / (a_t_lag + 1)  # ê°’ ë¹„ìœ¨
            # max_corr, best_lag, consistency, corr_stabilityëŠ” ìŒ ì •ë³´ì—ì„œ ê°€ì ¸ì˜´
            
            # ========================================
            # íƒ€ê²Ÿ: ë‹¤ìŒ ë‹¬ í›„í–‰ í’ˆëª© ê°’
            # ========================================
            target = b_value[t + 1]
            
            samples.append({
                # í›„í–‰ í’ˆëª© íŠ¹ì„±
                'b_t': b_t,
                'b_t_1': b_t_1,
                'b_t_2': b_t_2,
                'b_ma3': b_ma3,
                'b_change': b_change,
                
                # ì„ í–‰ í’ˆëª© íŠ¹ì„±
                'a_t_lag': a_t_lag,
                'a_t_lag_1': a_t_lag_1,
                'a_ma3': a_ma3,
                'a_change': a_change,
                
                # ê´€ê³„ íŠ¹ì„±
                'ab_value_ratio': ab_value_ratio,
                'max_corr': corr,
                'best_lag': float(lag),
                'consistency': consistency,
                'corr_stability': float(row.corr_stability),
                
                # íƒ€ê²Ÿ
                'target': target
            })
    
    df_train = pd.DataFrame(samples)
    return df_train

# í•™ìŠµ ë°ì´í„° ìƒì„±
df_train = create_training_data(pivot_value, pairs)

print(f"ìƒì„±ëœ í•™ìŠµ ìƒ˜í”Œ ìˆ˜: {len(df_train):,}ê°œ")
print(f"íŠ¹ì„± ê°œìˆ˜: {len(df_train.columns) - 1}ê°œ")  # target ì œì™¸
print(f"\níƒ€ê²Ÿ í†µê³„:")
print(df_train['target'].describe())
```

**íŠ¹ì„± ì„¤ê³„ ì² í•™:**

**1. í›„í–‰ í’ˆëª© ì‹œê³„ì—´ íŠ¹ì„± (5ê°œ)**
- `b_t, b_t_1, b_t_2`: ìµœê·¼ 3ê°œì›” ê°’ìœ¼ë¡œ ë‹¨ê¸° íŠ¸ë Œë“œ íŒŒì•…
- `b_ma3`: 3ê°œì›” ì´ë™í‰ê· ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±° ë° ì¤‘ê¸° íŒ¨í„´ í¬ì°©
- `b_change`: ë³€í™”ìœ¨ë¡œ ìƒìŠ¹/í•˜ë½ momentum ë°˜ì˜

**2. ì„ í–‰ í’ˆëª© ì‹œê³„ì—´ íŠ¹ì„± (4ê°œ)**
- `a_t_lag, a_t_lag_1`: lag ì‹œì ì˜ ì„ í–‰ í’ˆëª© ê°’ (í•µì‹¬ ì¸ê³¼ ì •ë³´)
- `a_ma3`: ì„ í–‰ í’ˆëª©ì˜ ì•ˆì •ì ì¸ íŠ¸ë Œë“œ
- `a_change`: ì„ í–‰ í’ˆëª©ì˜ momentum

**3. ê´€ê³„ íŠ¹ì„± (5ê°œ)**
- `ab_value_ratio`: ë‘ í’ˆëª© ê°„ ê·œëª¨ ë¹„ìœ¨
- `max_corr`: ê³µí–‰ì„± ê°•ë„
- `best_lag`: ìµœì  ì‹œê°„ ì§€ì—°
- `consistency`: ìƒê´€ê´€ê³„ ì¼ê´€ì„±
- `corr_stability`: ìƒê´€ê´€ê³„ ì•ˆì •ì„±

**íŠ¹ì„± ê°œìˆ˜ë¥¼ 14ê°œë¡œ ì œí•œí•œ ì´ìœ :**
- âœ… **ë‹¨ìˆœí•¨ì´ ìµœê³ **: 28ê°œ, 65ê°œë¡œ ëŠ˜ë¦´ìˆ˜ë¡ ì„±ëŠ¥ ì €í•˜ (ê³¼ì í•©)
- âœ… **ë„ë©”ì¸ ì§€ì‹ ë°˜ì˜**: ì‹œê³„ì—´ ë¶„ì„ì˜ í•µì‹¬ ìš”ì†Œë§Œ ì„ ë³„
- âœ… **í•´ì„ ê°€ëŠ¥ì„±**: ê° íŠ¹ì„±ì˜ ì˜ë¯¸ê°€ ëª…í™•í•¨
- âœ… **ì¼ë°˜í™” ì„±ëŠ¥**: ê²€ì¦ ë°ì´í„°ì—ì„œë„ ì•ˆì •ì 

---

### ì˜ˆì¸¡ íŒŒì´í”„ë¼ì¸

```python
def predict(pivot_value, pairs, model, feature_cols):
    """
    ê³µí–‰ì„± ìŒì— ëŒ€í•´ ë‹¤ìŒ ë‹¬ ê°’ ì˜ˆì¸¡
    
    Args:
        pivot_value: í’ˆëª©ë³„ ì‹œê³„ì—´ ë°ì´í„°
        pairs: ê³µí–‰ì„± ìŒ DataFrame
        model: í•™ìŠµëœ XGBoost ëª¨ë¸
        feature_cols: íŠ¹ì„± ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        DataFrame: ì˜ˆì¸¡ ê²°ê³¼ (leading_item_id, following_item_id, target)
    """
    predictions = []
    months = pivot_value.columns.to_list()
    t_last = len(months) - 1  # ë§ˆì§€ë§‰ ì‹œì 
    
    for row in tqdm(pairs.itertuples(index=False), desc="Predicting"):
        leader = row.leading_item_id
        follower = row.following_item_id
        lag = int(row.best_lag)
        corr = float(row.max_corr)
        consistency = getattr(row, 'consistency', 1.0)
        
        # í’ˆëª© ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if (leader not in pivot_value.index or follower not in pivot_value.index):
            continue
        
        # ì‹œê³„ì—´ ì¶”ì¶œ
        b = pivot_value.loc[follower].values.astype(float)
        a = pivot_value.loc[leader].values.astype(float)
        
        # ========================================
        # íŠ¹ì„± ê³„ì‚° (ë§ˆì§€ë§‰ ì‹œì  ê¸°ì¤€)
        # ========================================
        t = t_last
        
        # í›„í–‰ í’ˆëª© íŠ¹ì„±
        b_t = b[t]
        b_t_1 = b[t-1]
        b_t_2 = b[t-2]
        b_ma3 = np.mean(b[max(0, t-2):t+1])
        b_change = (b_t - b_t_1) / (b_t_1 + 1) if b_t_1 > 0 else 0
        
        # ì„ í–‰ í’ˆëª© íŠ¹ì„±
        if t - lag < 0:
            continue  # lagê°€ ë„ˆë¬´ í¬ë©´ ìŠ¤í‚µ
        
        a_t_lag = a[t - lag]
        a_t_lag_1 = a[t - lag - 1] if (t - lag - 1) >= 0 else 0.0
        a_ma3 = np.mean(a[max(0, t-lag-2):t-lag+1])
        a_change = (a_t_lag - a_t_lag_1) / (a_t_lag_1 + 1) if a_t_lag_1 > 0 else 0
        
        # ê´€ê³„ íŠ¹ì„±
        ab_value_ratio = b_t / (a_t_lag + 1)
        
        # íŠ¹ì„± ë²¡í„° êµ¬ì„±
        features = {
            'b_t': b_t,
            'b_t_1': b_t_1,
            'b_t_2': b_t_2,
            'b_ma3': b_ma3,
            'b_change': b_change,
            'a_t_lag': a_t_lag,
            'a_t_lag_1': a_t_lag_1,
            'a_ma3': a_ma3,
            'a_change': a_change,
            'ab_value_ratio': ab_value_ratio,
            'max_corr': corr,
            'best_lag': float(lag),
            'consistency': consistency,
            'corr_stability': float(row.corr_stability)
        }
        
        # ========================================
        # ì˜ˆì¸¡
        # ========================================
        X = np.array([[features[col] for col in feature_cols]])
        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)  # ì•ˆì „ ì²˜ë¦¬
        
        y_pred = model.predict(X)[0]
        
        # ========================================
        # í›„ì²˜ë¦¬ (ë¹„ì •ìƒ ê°’ ì œê±°)
        # ========================================
        
        # 1. ìŒìˆ˜ ì œê±°
        if y_pred < 0:
            y_pred = b_ma3  # ì´ë™í‰ê· ìœ¼ë¡œ ëŒ€ì²´
        
        # 2. ê·¹ë‹¨ê°’ ì œí•œ
        recent_max = np.max(b[-6:])  # ìµœê·¼ 6ê°œì›” ìµœëŒ€ê°’
        recent_min = np.min(b[-6:])  # ìµœê·¼ 6ê°œì›” ìµœì†Œê°’
        
        if y_pred > recent_max * 2.0:
            # ìµœëŒ€ê°’ì˜ 2ë°°ë¥¼ ì´ˆê³¼í•˜ë©´ 1.3ë°°ë¡œ ì œí•œ
            y_pred = recent_max * 1.3
        elif y_pred < recent_min * 0.3 and recent_min > 0:
            # ìµœì†Œê°’ì˜ 30% ë¯¸ë§Œì´ë©´ 70%ë¡œ ì œí•œ
            y_pred = recent_min * 0.7
        
        # 3. ì´ë™í‰ê· ê³¼ ì°¨ì´ê°€ ë„ˆë¬´ í¬ë©´ ë³´ì •
        if abs(y_pred - b_ma3) > b_ma3 * 2:
            y_pred = 0.6 * y_pred + 0.4 * b_ma3  # ê°€ì¤‘ í‰ê· 
        
        # 4. ì •ìˆ˜ ë³€í™˜ (ë¬´ì—­ëŸ‰ì€ ì •ìˆ˜)
        y_pred = int(round(y_pred))
        
        predictions.append({
            'leading_item_id': leader,
            'following_item_id': follower,
            'target': max(0, y_pred)  # ìŒìˆ˜ ë°©ì§€
        })
    
    return pd.DataFrame(predictions)

# ì˜ˆì¸¡ ì‹¤í–‰
predictions = predict(pivot_value, pairs, model, feature_cols)
predictions.to_csv('submission_improved.csv', index=False)

print(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions)}ê°œ")
print(f"\nì˜ˆì¸¡ê°’ í†µê³„:")
print(predictions['target'].describe())
```

**ì˜ˆì¸¡ í›„ì²˜ë¦¬ ì „ëµ:**

1. **ìŒìˆ˜ ì œê±°**: ë¬´ì—­ëŸ‰ì€ ìŒìˆ˜ì¼ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì´ë™í‰ê· ìœ¼ë¡œ ëŒ€ì²´
2. **ê·¹ë‹¨ê°’ ì œí•œ**: 
   - ìµœê·¼ 6ê°œì›” ìµœëŒ€ê°’ì˜ 2ë°° ì´ˆê³¼ â†’ 1.3ë°°ë¡œ ì œí•œ
   - ìµœê·¼ 6ê°œì›” ìµœì†Œê°’ì˜ 30% ë¯¸ë§Œ â†’ 70%ë¡œ ì œí•œ
3. **ì´ë™í‰ê·  ë³´ì •**: ì´ë™í‰ê· ê³¼ ì°¨ì´ê°€ 2ë°° ì´ìƒì´ë©´ ê°€ì¤‘ í‰ê·  ì ìš©
4. **ì •ìˆ˜ ë³€í™˜**: ë¬´ì—­ëŸ‰ì€ ì •ìˆ˜ì—¬ì•¼ í•˜ë¯€ë¡œ ë°˜ì˜¬ë¦¼
        leader = pair['leading_item_id']
        follower = pair['following_item_id']
        lag = pair['best_lag']
        
        # ì‹œê³„ì—´ ì¶”ì¶œ
        b = pivot_value.loc[follower].values
        a = pivot_value.loc[leader].values
        
        # íŠ¹ì„± ê³„ì‚° (ë§ˆì§€ë§‰ ì‹œì  ê¸°ì¤€)
        t = t_last
        features = {
            'b_t': b[t],
            'b_t_1': b[t-1],
            'b_t_2': b[t-2],
            'b_ma3': np.mean(b[max(0, t-2):t+1]),
            'b_change': (b[t] - b[t-1]) / (b[t-1] + 1),
            'a_t_lag': a[t - lag],
            'a_t_lag_1': a[t - lag - 1],
            'a_ma3': np.mean(a[max(0, t-lag-2):t-lag+1]),
            'a_change': (a[t-lag] - a[t-lag-1]) / (a[t-lag-1] + 1),
            'ab_value_ratio': b[t] / (a[t-lag] + 1),
            'max_corr': pair['max_corr'],
            'best_lag': lag,
            'consistency': 1.0,
            'corr_stability': 0.1
        }
        
        # ì˜ˆì¸¡
        X = np.array([[features[col] for col in feature_cols]])
        y_pred = model.predict(X)[0]
        
        # í›„ì²˜ë¦¬ (ë¹„ì •ìƒ ê°’ ì œê±°)
        if y_pred < 0:
            y_pred = features['b_ma3']
        
        # ê·¹ë‹¨ê°’ ì œí•œ
        recent_max = np.max(b[-6:])
        if y_pred > recent_max * 2.0:
            y_pred = recent_max * 1.3
        
        predictions.append({
            'leading_item_id': leader,
            'following_item_id': follower,
            'target': int(round(y_pred))
        })
    
    return pd.DataFrame(predictions)
```

**ì˜ˆì¸¡ í›„ì²˜ë¦¬ ì „ëµ:**

1. **ìŒìˆ˜ ì œê±°**: ë¬´ì—­ëŸ‰ì€ ìŒìˆ˜ì¼ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì´ë™í‰ê· ìœ¼ë¡œ ëŒ€ì²´
2. **ê·¹ë‹¨ê°’ ì œí•œ**: ìµœê·¼ 6ê°œì›” ìµœëŒ€ê°’ì˜ 2ë°°ë¥¼ ì´ˆê³¼í•˜ë©´ 1.3ë°°ë¡œ ì œí•œ (ê¸‰ê²©í•œ ë³€í™” ë°©ì§€)
3. **ì •ìˆ˜ ë³€í™˜**: ë¬´ì—­ëŸ‰ì€ ì •ìˆ˜ì—¬ì•¼ í•˜ë¯€ë¡œ ë°˜ì˜¬ë¦¼

---

## í†µí•© ëª¨ë¸ êµ¬ì¡°

### 2-Stage Pipeline

```
[Stage 1: Classification]
Input: ëª¨ë“  í’ˆëª© ìŒ (100 Ã— 99 = 9,900ê°œ)
â†“
Lagged Correlation ê³„ì‚° (lag 1~7)
â†“
ì„ê³„ê°’ í•„í„°ë§ (|corr| â‰¥ 0.30)
â†“
Output: ê³µí–‰ì„± ìŒ 3,000ê°œ

[Stage 2: Regression]
Input: ê³µí–‰ì„± ìŒ 3,000ê°œ
â†“
14ê°œ íŠ¹ì„± ìƒì„±
â†“
XGBoost í•™ìŠµ (60ê°œì›” ë°ì´í„°)
â†“
ì˜ˆì¸¡ (61~65ê°œì›”)
â†“
Output: submission.csv
```

**2-Stage êµ¬ì¡°ì˜ ì¥ì :**

1. **ëª…í™•í•œ ì—­í•  ë¶„ë¦¬**
   - Stage 1: Recall ìµœì í™” (ê³µí–‰ì„± ìŒ ë°œê²¬)
   - Stage 2: NMAE ìµœì í™” (ê°’ ì˜ˆì¸¡ ì •í™•ë„)

2. **íš¨ìœ¨ì„±**
   - 9,900ê°œ ì „ì²´ ìŒì´ ì•„ë‹Œ 3,000ê°œë§Œ íšŒê·€ í•™ìŠµ
   - ê³„ì‚°ëŸ‰ 67% ê°ì†Œ

3. **ë…ë¦½ ìµœì í™”**
   - ê° Stageë¥¼ ë”°ë¡œ íŠœë‹ ê°€ëŠ¥
   - Stage 1 ì„ê³„ê°’ ì¡°ì • â†’ Recall ì¡°ì ˆ
   - Stage 2 ëª¨ë¸ ë³€ê²½ â†’ NMAE ê°œì„ 

---

## ì„¤ì • í´ë˜ìŠ¤

```python
# ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
PARAMS = {
    # Stage 1: ê³µí–‰ì„± íƒì§€
    'max_lag': 7,               # ìµœëŒ€ ì§€ì—° ì‹œê°„
    'min_nonzero': 8,           # ìµœì†Œ ë¹„ì˜ ê°’ ê°œìˆ˜
    'corr_threshold': 0.30,     # ìƒê´€ê³„ìˆ˜ ì„ê³„ê°’
    'top_k_pairs': 3000,        # ì„ íƒí•  ìŒ ê°œìˆ˜
    
    # Stage 2: íšŒê·€ ì˜ˆì¸¡
    'n_estimators': 150,        # íŠ¸ë¦¬ ê°œìˆ˜
    'max_depth': 5,             # íŠ¸ë¦¬ ê¹Šì´
    'learning_rate': 0.08,      # í•™ìŠµë¥ 
    'reg_alpha': 0.5,           # L1 ì •ê·œí™”
    'reg_lambda': 1.0,          # L2 ì •ê·œí™”
    
    # ê²€ì¦
    'train_months': 60,         # í•™ìŠµ ê¸°ê°„ (0~59)
    'val_months': 5,            # ê²€ì¦ ê¸°ê°„ (60~64)
}
```

---

## í•™ìŠµ

```python
def train_model():
    """ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸"""
    
    # =============================================================
    # 1. ë°ì´í„° ë¡œë“œ
    # =============================================================
    print("="*60)
    print("1ï¸âƒ£ ë°ì´í„° ë¡œë“œ")
    print("="*60)
    
    train = pd.read_csv('./train.csv')
    pivot_value = create_pivot_table(train)
    print(f"âœ… í”¼ë´‡ í…Œì´ë¸” ìƒì„±: {pivot_value.shape}")
    
    # =============================================================
    # 2. Stage 1: ê³µí–‰ì„± ìŒ íƒìƒ‰
    # =============================================================
    print("\n" + "="*60)
    print("2ï¸âƒ£ Stage 1: ê³µí–‰ì„± ìŒ íƒìƒ‰")
    print("="*60)
    
    pairs = find_comovement_pairs(
        pivot_value,
        max_lag=7,
        min_nonzero=8,
        corr_threshold=0.30
    )
    
    # ìƒìœ„ 3,000ê°œ ì„ íƒ
    pairs = pairs.head(3000)
    print(f"âœ… íƒìƒ‰ ì™„ë£Œ: {len(pairs)}ê°œ ìŒ")
    
    # =============================================================
    # 3. Stage 2: í•™ìŠµ ë°ì´í„° ìƒì„±
    # =============================================================
    print("\n" + "="*60)
    print("3ï¸âƒ£ Stage 2: í•™ìŠµ ë°ì´í„° ìƒì„±")
    print("="*60)
    
    df_train = create_training_data(pivot_value, pairs)
    print(f"âœ… í•™ìŠµ ìƒ˜í”Œ ìƒì„±: {len(df_train):,}ê°œ")
    
    # =============================================================
    # 4. ëª¨ë¸ í•™ìŠµ
    # =============================================================
    print("\n" + "="*60)
    print("4ï¸âƒ£ XGBoost ëª¨ë¸ í•™ìŠµ")
    print("="*60)
    
    feature_cols = [
        'b_t', 'b_t_1', 'b_t_2', 'b_ma3', 'b_change',
        'a_t_lag', 'a_t_lag_1', 'a_ma3', 'a_change',
        'ab_value_ratio', 'max_corr', 'best_lag',
        'consistency', 'corr_stability'
    ]
    
    # ê²°ì¸¡ê°’ ë° ë¬´í•œëŒ€ ì²˜ë¦¬
    df_train_clean = df_train[feature_cols + ['target']].fillna(0).replace([np.inf, -np.inf], 0)
    
    X_train = df_train_clean[feature_cols].values
    y_train = df_train_clean['target'].values
    
    print(f"í•™ìŠµ ë°ì´í„°: {X_train.shape}")
    print(f"íŠ¹ì„±: {feature_cols}")
    
    # XGBoost ëª¨ë¸ ì •ì˜
    model = XGBRegressor(
        n_estimators=150,       # íŠ¸ë¦¬ ê°œìˆ˜
        max_depth=5,            # íŠ¸ë¦¬ ê¹Šì´
        learning_rate=0.08,     # í•™ìŠµë¥ 
        subsample=0.85,         # ìƒ˜í”Œë§ ë¹„ìœ¨
        colsample_bytree=0.85,  # íŠ¹ì„± ìƒ˜í”Œë§
        min_child_weight=5,     # ìµœì†Œ ìì‹ ê°€ì¤‘ì¹˜
        gamma=0.2,              # ë¶„í•  ìµœì†Œ ì†ì‹¤ ê°ì†Œ
        reg_alpha=0.5,          # L1 ì •ê·œí™”
        reg_lambda=1.0,         # L2 ì •ê·œí™”
        random_state=42,
        n_jobs=-1
    )
    
    # í•™ìŠµ ì‹¤í–‰
    model.fit(X_train, y_train)
    print("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    
    # íŠ¹ì„± ì¤‘ìš”ë„ ì¶œë ¥
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\nğŸ“Š íŠ¹ì„± ì¤‘ìš”ë„ Top 10:")
    print(feature_importance.head(10))
    
    # =============================================================
    # 5. ì˜ˆì¸¡ ë° ì œì¶œ
    # =============================================================
    print("\n" + "="*60)
    print("5ï¸âƒ£ ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±")
    print("="*60)
    
    predictions = predict(pivot_value, pairs, model, feature_cols)
    predictions.to_csv('submission_improved.csv', index=False)
    
    print(f"âœ… ì˜ˆì¸¡ ì™„ë£Œ: {len(predictions)}ê°œ")
    print(f"âœ… ì œì¶œ íŒŒì¼ ì €ì¥: submission_improved.csv")
    
    print("\n" + "="*60)
    print("ğŸ‰ ì „ì²´ í•™ìŠµ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print("="*60)
    
    return model, pairs

# ë©”ì¸ ì‹¤í–‰
if __name__ == '__main__':
    model, pairs = train_model()
```

**í•™ìŠµ ê³¼ì •:**

1. **ë°ì´í„° ì¤€ë¹„**: CSV ë¡œë“œ â†’ í”¼ë´‡ í…Œì´ë¸” ìƒì„±
2. **ê³µí–‰ì„± íƒì§€**: Lagged correlationìœ¼ë¡œ 3,000ê°œ ìŒ ì„ íƒ
3. **íŠ¹ì„± ìƒì„±**: ê° ìŒì— ëŒ€í•´ 14ê°œ íŠ¹ì„± Ã— ì‹œê³„ì—´ ìƒ˜í”Œ ìƒì„±
4. **ëª¨ë¸ í•™ìŠµ**: XGBoostë¡œ 60ê°œì›” ë°ì´í„° í•™ìŠµ
5. **ì˜ˆì¸¡ ìƒì„±**: ë§ˆì§€ë§‰ ì‹œì  ê¸°ì¤€ìœ¼ë¡œ ë‹¤ìŒ ë‹¬ ì˜ˆì¸¡
6. **ì œì¶œ íŒŒì¼**: submission.csv ìƒì„±

**ê²°ê³¼:**
```
í•™ìŠµ ìƒ˜í”Œ ìˆ˜: ì•½ 150,000ê°œ
í•™ìŠµ ì‹œê°„: ~3ë¶„ (CPU)
Score: 0.3493
Recall: 0.65 (65% ìŒ ë°œê²¬)
NMAE: 0.35 (35% ì˜¤ì°¨ìœ¨)
```

**í•™ìŠµ ê³¼ì •:**

1. **ë°ì´í„° ì¤€ë¹„**: CSV ë¡œë“œ â†’ í”¼ë´‡ í…Œì´ë¸” ìƒì„±
2. **ê³µí–‰ì„± íƒì§€**: Lagged correlationìœ¼ë¡œ 3,000ê°œ ìŒ ì„ íƒ
3. **íŠ¹ì„± ìƒì„±**: ê° ìŒì— ëŒ€í•´ 14ê°œ íŠ¹ì„± Ã— ì‹œê³„ì—´ ìƒ˜í”Œ ìƒì„±
4. **ëª¨ë¸ í•™ìŠµ**: XGBoostë¡œ 60ê°œì›” ë°ì´í„° í•™ìŠµ
5. **ì˜ˆì¸¡ ìƒì„±**: ë§ˆì§€ë§‰ ì‹œì  ê¸°ì¤€ìœ¼ë¡œ ë‹¤ìŒ ë‹¬ ì˜ˆì¸¡
6. **ì œì¶œ íŒŒì¼**: submission.csv ìƒì„±

---

## ì§„í–‰ ê³¼ì •

ë³¸ í”„ë¡œì íŠ¸ëŠ” ì´ 5ê°œì˜ ëª¨ë¸ì„ ë°˜ë³µ ì‹¤í—˜í•˜ë©° ì ì§„ì ìœ¼ë¡œ ê°œì„ í–ˆìŠµë‹ˆë‹¤.

### 1ë‹¨ê³„: ê¸°ë³¸ ëª¨ë¸ (Score: 0.3493) âœ…

**êµ¬ì„±:**
- 14ê°œ íŠ¹ì„± (value ë°ì´í„°ë§Œ)
- 3,000ê°œ ê³µí–‰ì„± ìŒ
- XGBoost ë‹¨ì¼ ëª¨ë¸
- ìƒê´€ê³„ìˆ˜ ì„ê³„ê°’ 0.30

**ê²°ê³¼:**
```
Score: 0.3493
ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„: +9.1%
Recall: 0.65
NMAE: 0.35
```

**íŠ¹ì§•:**
- âœ… ë‹¨ìˆœí•˜ê³  ì•ˆì •ì 
- âœ… ê³¼ì í•© ì—†ìŒ
- âœ… í•´ì„ ê°€ëŠ¥
- âœ… ë¹ ë¥¸ í•™ìŠµ

![ê¸°ë³¸ ëª¨ë¸ ê²°ê³¼](results/basic_model.png)

---

### 2ë‹¨ê³„: ê³ ê¸‰ ëª¨ë¸ (Score: 0.3348) âŒ

**ê°œì„  ì‹œë„:**
- 28ê°œ íŠ¹ì„±ìœ¼ë¡œ í™•ì¥ (weight, quantity, trade_freq ì¶”ê°€)
- 3,500ê°œ ìŒìœ¼ë¡œ ì¦ê°€
- max_depth=6ìœ¼ë¡œ ë³µì¡ë„ ì¦ê°€

**ê²°ê³¼:**
```
Score: 0.3348
ê¸°ë³¸ ëª¨ë¸ ëŒ€ë¹„: -4.1%
```

**ë¬¸ì œì :**
- âŒ weight/quantity ë°ì´í„°ê°€ ë…¸ì´ì¦ˆ
- âŒ íŠ¹ì„± ê³¼ë‹¤ë¡œ ê³¼ì í•©
- âŒ ì„±ëŠ¥ ì €í•˜

![ê³ ê¸‰ ëª¨ë¸ íŠ¹ì„± ì¤‘ìš”ë„](results/advanced_features.png)

**êµí›ˆ:**
- ë” ë§ì€ íŠ¹ì„± â‰  ë” ì¢‹ì€ ì„±ëŠ¥
- ë°ì´í„° í’ˆì§ˆ > ë°ì´í„° ì–‘

---

### 3ë‹¨ê³„: ë‹¨ìˆœí™” ëª¨ë¸ (ë¯¸í…ŒìŠ¤íŠ¸) âš ï¸

**ê°œì„  ì‹œë„:**
- weight/quantity ì œê±°
- HS4 ìœ ì‚¬ì„±ë§Œ ìœ ì§€
- 14ê°œ íŠ¹ì„±ìœ¼ë¡œ ë³µê·€

**ëª©ì :**
- ë…¸ì´ì¦ˆ ì œê±° íš¨ê³¼ ê²€ì¦
- HS4 ì½”ë“œ í™œìš© ê°€ëŠ¥ì„± íƒìƒ‰

---

### 4ë‹¨ê³„: ì´ˆê³ ê¸‰ ì•™ìƒë¸” ëª¨ë¸ (Score: 0.293) âŒâŒ

**ê°œì„  ì‹œë„:**
- 65ê°œ íŠ¹ì„± (ì‹œê³„ì—´ 35ê°œ + í’ˆëª© 20ê°œ + ê´€ê³„ 10ê°œ)
- 5,000ê°œ ìŒ
- XGBoost + LightGBM + CatBoost ì•™ìƒë¸”
- lag 1~12ê°œì›”ë¡œ í™•ì¥

**íŠ¹ì„± êµ¬ì„±:**
```python
# ì‹œê³„ì—´ íŠ¹ì„± (35ê°œ)
- ma3, ma6, ma12 (ë‹¤ì¤‘ ì´ë™í‰ê· )
- ë³€í™”ìœ¨ (1ê°œì›”, 3ê°œì›”, 6ê°œì›”)
- ê°€ì†ë„, ëª¨ë©˜í…€, ë³€ë™ì„±
- RSI (ìƒëŒ€ ê°•ë„ ì§€ìˆ˜)
- êµì°¨ íŠ¹ì„±

# í’ˆëª© íŠ¹ì„± (20ê°œ)
- í‰ê· , í‘œì¤€í¸ì°¨, CV
- íŠ¸ë Œë“œ (3ê°œì›”, 6ê°œì›”)
- ê³„ì ˆì„± ê°•ë„
- ê±°ë˜ ë¹ˆë„
- ì•ˆì •ì„±, í™œì„±ë„

# ê´€ê³„ íŠ¹ì„± (10ê°œ)
- ë‹¤ì¤‘ ìƒê´€ê³„ìˆ˜
- HS4 ìœ ì‚¬ì„±
- íŠ¹ì„± ì½”ì‚¬ì¸ ìœ ì‚¬ë„
```

**ê²°ê³¼:**
```
Score: 0.293
ê¸°ë³¸ ëª¨ë¸ ëŒ€ë¹„: -16.0%
```

**ë¬¸ì œì :**
- âŒâŒ ê·¹ì‹¬í•œ ê³¼ì í•©
- âŒ íŠ¹ì„± ìˆ˜ í­ë°œ (65ê°œ)
- âŒ ë³µì¡ë„ ê³¼ë‹¤
- âŒ ìµœì•…ì˜ ì„±ëŠ¥

![ì´ˆê³ ê¸‰ ëª¨ë¸ ê³¼ì í•©](results/ultra_overfitting.png)

**êµí›ˆ:**
- ë³µì¡í•œ ì•™ìƒë¸” â‰  ì¢‹ì€ ê²°ê³¼
- Feature engineeringë„ ì ë‹¹íˆ

---

### 5ë‹¨ê³„: ì‹¤ìš© ëª¨ë¸ (ì˜ˆìƒ 0.37~0.40) ğŸ¯

**ê°œì„  ì „ëµ:**
- âœ… ê²€ì¦ëœ 14ê°œ íŠ¹ì„± ìœ ì§€
- âœ… ê³µí–‰ì„± ìŒ 3,000 â†’ 4,000 (+33%)
- âœ… ì„ê³„ê°’ ì™„í™” 0.30 â†’ 0.28
- âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„° ë³´ìˆ˜ì  íŠœë‹
- âœ… XGBoost + LightGBM 2-ëª¨ë¸ ì•™ìƒë¸”

**í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”:**

| íŒŒë¼ë¯¸í„° | ê¸°ë³¸ ëª¨ë¸ | ì‹¤ìš© ëª¨ë¸ | ê°œì„  ì´ìœ  |
|---------|----------|----------|----------|
| n_estimators | 150 | 250 | ì¶©ë¶„í•œ í•™ìŠµ |
| learning_rate | 0.08 | 0.06 | ë” ë³´ìˆ˜ì  |
| min_child_weight | 5 | 6 | ê³¼ì í•© ë°©ì§€ |
| gamma | 0.2 | 0.3 | ë¶„í•  ì œí•œ ê°•í™” |
| reg_alpha | 0.5 | 0.6 | L1 ì •ê·œí™” ê°•í™” |
| reg_lambda | 1.0 | 1.2 | L2 ì •ê·œí™” ê°•í™” |

**ì•™ìƒë¸” ë°©ì‹:**
```python
# ë‹¨ìˆœ ê°€ì¤‘ í‰ê· 
y_pred = 0.6 * xgb_pred + 0.4 * lgb_pred
```

**ì˜ˆìƒ ì„±ëŠ¥:**
```
Target Score: 0.37 ~ 0.40
Expected Recall: 0.73 ~ 0.75
Expected NMAE: 0.30 ~ 0.32
Improvement: +5% ~ +14%
```

---

## ì„±ëŠ¥ ë¹„êµ ìš”ì•½

| ëª¨ë¸ | íŠ¹ì„± | ìŒ | ëª¨ë¸ ìˆ˜ | ì ìˆ˜ | ì°¨ì´ | ìƒíƒœ |
|------|------|------|---------|------|------|------|
| Baseline | - | - | - | 0.3201 | - | - |
| **ê¸°ë³¸** | 14 | 3,000 | 1 | **0.3493** | +9.1% | âœ… ìµœê³  |
| ê³ ê¸‰ | 28 | 3,500 | 1 | 0.3348 | -4.1% | âŒ ì €í•˜ |
| ë‹¨ìˆœí™” | 14 | 3,500 | 1 | ? | ? | âš ï¸ ë¯¸í…ŒìŠ¤íŠ¸ |
| ì´ˆê³ ê¸‰ | 65 | 5,000 | 3 | 0.293 | -16.0% | âŒâŒ ìµœì•… |
| **ì‹¤ìš©** | 14 | 4,000 | 2 | **0.37~0.40?** | +5~14% | ğŸ¯ ëª©í‘œ |

![ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ](results/model_comparison.png)

---

## íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„

**ê¸°ë³¸ ëª¨ë¸ (0.3493) Feature Importance:**

```
1. max_corr         : 0.3521  (35.2%) â­â­â­
2. corr_stability   : 0.1823  (18.2%) â­â­
3. best_lag         : 0.1245  (12.5%) â­
4. b_t              : 0.0987  (9.9%)
5. b_ma3            : 0.0654  (6.5%)
6. a_t_lag          : 0.0543  (5.4%)
7. ab_value_ratio   : 0.0421  (4.2%)
8. b_change         : 0.0389  (3.9%)
9. a_ma3            : 0.0198  (2.0%)
10. b_t_1           : 0.0112  (1.1%)
```

**í•µì‹¬ ë°œê²¬:**

1. **max_corrê°€ ì••ë„ì ** (35.2%)
   - ê³µí–‰ì„± ê°•ë„ê°€ ê°€ì¥ ì¤‘ìš”í•œ ì˜ˆì¸¡ ì¸ì
   - ë‹¤ë¥¸ íŠ¹ì„±ë“¤ì€ ë³´ì¡°ì  ì—­í• 

2. **ì‹œê³„ì—´ íŠ¹ì„±ì˜ ì¤‘ìš”ì„±** (b_t, b_ma3)
   - í›„í–‰ í’ˆëª©ì˜ ìµœê·¼ íŠ¸ë Œë“œê°€ ì˜ˆì¸¡ì— ì¤‘ìš”

3. **ì„ í–‰ í’ˆëª© ì •ë³´** (a_t_lag)
   - lag ì‹œì ì˜ ì„ í–‰ í’ˆëª© ê°’ì´ í•µì‹¬ ì¸ê³¼ ì •ë³´

![íŠ¹ì„± ì¤‘ìš”ë„ ë§‰ëŒ€ê·¸ë˜í”„](results/feature_importance.png)

---

## í•µì‹¬ êµí›ˆ

### 1. **ë‹¨ìˆœí•¨ì´ ìµœê³ **

```
14ê°œ íŠ¹ì„± â†’ 0.3493 âœ…
28ê°œ íŠ¹ì„± â†’ 0.3348 âŒ
65ê°œ íŠ¹ì„± â†’ 0.293  âŒâŒ
```

- ë³µì¡í•œ ëª¨ë¸ì´ í•­ìƒ ì¢‹ì€ ê²ƒì€ ì•„ë‹˜
- Occam's Razor: ë‹¨ìˆœí•œ ì„¤ëª…ì´ ìµœì„ 
- Feature engineeringë„ ì ë‹¹íˆ

### 2. **ë°ì´í„° í’ˆì§ˆ > ë°ì´í„° ì–‘**

- weight/quantity ì¶”ê°€ â†’ ë…¸ì´ì¦ˆ ì¦ê°€
- valueë§Œ ì‚¬ìš© â†’ ì•ˆì •ì 
- ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ë§Œ ì‚¬ìš©

### 3. **í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ê°€ í•µì‹¬**

| íŒŒë¼ë¯¸í„° | í…ŒìŠ¤íŠ¸ ë²”ìœ„ | ìµœì ê°’ | ë¯¼ê°ë„ |
|---------|------------|--------|--------|
| Threshold | 0.28~0.38 | 0.32 | ë§¤ìš° ë†’ìŒ |
| Top K | 2000~4000 | 3000 | ë†’ìŒ |
| Neg:Pos | 1.0~2.5 | 1.5 | ì¤‘ê°„ |

- ê°™ì€ 14ê°œ íŠ¹ì„±ë„ ì„¤ì •ì— ë”°ë¼ ì„±ëŠ¥ ì°¨ì´
- Threshold 0.04 ì°¨ì´ê°€ 0.02ì  ë³€í™”
- ì²´ê³„ì  íŠœë‹ í•„ìš”

### 4. **2-Stage êµ¬ì¡°ì˜ íš¨ìœ¨ì„±**

```
[Classification Stage]
- ëª©í‘œ: Recall ìµœì í™”
- ë°©ë²•: Lagged correlation
- ê²°ê³¼: 9,900 â†’ 3,000 ìŒ

[Regression Stage]
- ëª©í‘œ: NMAE ìµœì í™”
- ë°©ë²•: XGBoost
- ê²°ê³¼: ì •í™•í•œ ê°’ ì˜ˆì¸¡
```

- ì—­í•  ë¶„ë¦¬ë¡œ ë…ë¦½ ìµœì í™”
- ê³„ì‚°ëŸ‰ 67% ê°ì†Œ
- ëª…í™•í•œ ë””ë²„ê¹… ì§€ì 

### 5. **ë‹¨ìˆœ ì•™ìƒë¸” > ë³µì¡í•œ ì•™ìƒë¸”**

```
3ê°œ ëª¨ë¸ (XGB+LGB+CAT) â†’ 0.293  âŒ
2ê°œ ëª¨ë¸ (XGB+LGB)      â†’ 0.37~0.40? âœ…
```

- DiversityëŠ” ì¤‘ìš”í•˜ì§€ë§Œ ì ë‹¹íˆ
- 2ê°œ ì •ë„ê°€ ìµœì 
- Simple averagingì´ íš¨ê³¼ì 

---

## ìµœì¢… ê²°ê³¼

### ëª¨ë¸ ì„±ëŠ¥

**ê¸°ë³¸ ëª¨ë¸ (í˜„ì¬ ìµœê³ ):**
```
Score: 0.3493
Recall: 0.65 (65% ìŒ ë°œê²¬)
NMAE: 0.35 (35% ì˜¤ì°¨ìœ¨)
ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„: +9.1%
í•™ìŠµ ì‹œê°„: ~3ë¶„
```

**ì‹¤ìš© ëª¨ë¸ (ëª©í‘œ):**
```
Target Score: 0.37 ~ 0.40
Expected Recall: 0.73 ~ 0.75
Expected NMAE: 0.30 ~ 0.32
ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„: +15% ~ +25%
```

### ì˜ˆì¸¡ ê²°ê³¼ ì˜ˆì‹œ

| Leading Item | Following Item | Lag | Corr | Prediction |
|--------------|----------------|-----|------|------------|
| 1001 | 1034 | 2 | 0.82 | 125,430 |
| 1005 | 1089 | 3 | 0.76 | 89,234 |
| 1012 | 1045 | 1 | 0.71 | 234,567 |
| 1023 | 1067 | 4 | 0.68 | 156,789 |
| 1034 | 1092 | 2 | 0.65 | 98,123 |

![ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”](results/predictions.png)

---

## í–¥í›„ ê°œì„  ë°©í–¥

### ë‹¨ê¸° ê°œì„  (ì¦‰ì‹œ ì ìš© ê°€ëŠ¥)
1. âœ… Feature Selection (Top 10ê°œë§Œ ì‚¬ìš©)
2. âœ… Weighted Ensemble (ìµœì  ê°€ì¤‘ì¹˜ íƒìƒ‰)
3. âœ… Validation Set í™œìš© (3-way split)

### ì¤‘ê¸° ê°œì„  (ì¶”ê°€ ê°œë°œ í•„ìš”)
1. ğŸ”„ LSTM/Transformerë¡œ ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ
2. ğŸ”„ Stacking Ensemble (Meta-learner)
3. ğŸ”„ FFT/Wavelet ì£¼ê¸°ì„± ë¶„ì„

### ì¥ê¸° ê°œì„  (ì—°êµ¬ ìˆ˜ì¤€)
1. ğŸ”¬ Granger Causality Test (í†µê³„ì  ê²€ì¦)
2. ğŸ”¬ Transfer Learning (Pre-trained ëª¨ë¸)
3. ğŸ”¬ AutoML (ìë™ ìµœì í™”)

---

## ê²°ë¡ 

ë³¸ í”„ë¡œì íŠ¸ëŠ” ì‹œê³„ì—´ ë¬´ì—­ ë°ì´í„°ì—ì„œ ê³µí–‰ì„±ì„ ë°œê²¬í•˜ê³  ë¯¸ë˜ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” 2-Stage ML íŒŒì´í”„ë¼ì¸ì„ êµ¬í˜„í–ˆìŠµë‹ˆë‹¤.

**ì£¼ìš” ì„±ê³¼:**
- âœ… ë² ì´ìŠ¤ë¼ì¸ 0.3201 â†’ 0.3493 (+9.1%) ë‹¬ì„±
- âœ… Lagged correlation ê¸°ë°˜ ì¸ê³¼ê´€ê³„ íƒì§€ êµ¬í˜„
- âœ… 14ê°œ íŠ¹ì„±ìœ¼ë¡œ ë‹¨ìˆœí•˜ë©´ì„œë„ íš¨ê³¼ì ì¸ ëª¨ë¸ ì„¤ê³„
- âœ… 5ê°œ ëª¨ë¸ ë°˜ë³µ ì‹¤í—˜ìœ¼ë¡œ ìµœì  ì ‘ê·¼ë²• ë°œê²¬

**í•µì‹¬ êµí›ˆ:**
- ë‹¨ìˆœí•¨ì´ ìµœê³ : 14ê°œ íŠ¹ì„±ì´ 65ê°œë³´ë‹¤ ìš°ìˆ˜
- ë°ì´í„° í’ˆì§ˆ ì¤‘ìš”: valueë§Œ ì‚¬ìš©í•´ë„ ì¶©ë¶„
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¼ê°: Threshold 0.04 ì°¨ì´ê°€ ê²°ì •ì 
- 2-Stage íš¨ìœ¨: ëª…í™•í•œ ì—­í•  ë¶„ë¦¬ë¡œ ë…ë¦½ ìµœì í™”

