# 21101170 ê¹€ë³‘í˜¸ ?„ë¡œ?íŠ¸

## ë¬¸ì œ ?•ì˜

ë³??„ë¡œ?íŠ¸??100ê°??˜ì… ?ˆëª©???”ë³„ ë¬´ì—­ ?°ì´?°ë? ë¶„ì„?˜ì—¬ ê³µí–‰??Comovement)???ˆëŠ” ?ˆëª© ?ì„ ?ë³„?˜ê³ , ? í–‰ ?ˆëª©???•ë³´ë¡??„í–‰ ?ˆëª©???¤ìŒ ??ë¬´ì—­?‰ì„ ?ˆì¸¡?˜ëŠ” ?œê³„???ˆì¸¡ ë¬¸ì œ?…ë‹ˆ?? 

**?‰ê? ì§€??**
- Score = 0.5 Ã— Recall + 0.5 Ã— (1 - NMAE)
- Recall: ?¤ì œ ê³µí–‰????ì¤?ëª¨ë¸??ë°œê²¬??ë¹„ìœ¨
- NMAE: ?ˆì¸¡ê°’ê³¼ ?¤ì œê°?ê°„ì˜ ?•ê·œ?”ëœ ?‰ê·  ?ˆë? ?¤ì°¨

**ëª©í‘œ:**
- ë² ì´?¤ë¼?? 0.3201
- ìµœì¢… ëª©í‘œ: 0.40 ?´ìƒ

---

## ì½”ë“œ ë°?ëª¨ë¸ êµ¬ì¡° ? íƒ ë°°ê²½

### ê¸°ì¡´ ?‘ê·¼ë²•ì˜ ?œê³„

?œê³„???ˆì¸¡ ë¬¸ì œ?ì„œ ?„í†µ?ìœ¼ë¡??¬ìš©?˜ëŠ” ?‘ê·¼ë²•ë“¤?€ ê°ê° ?œê³„ë¥?ê°€ì§€ê³??ˆìŠµ?ˆë‹¤.

#### 1. **CNN ê¸°ë°˜ ëª¨ë¸???œê³„**

**?¹ì§•:**
- ?©ì„±ê³?Convolution) ?°ì‚°???µí•´ ì§€?? (local) ?¨í„´ ì¶”ì¶œ
- Receptive field ?¬ê¸°???œí•œ?˜ì–´ ê·¼ê±°ë¦??•ë³´?ë§Œ ì§‘ì¤‘

**?œê³„??**
- ??**?œí•œ???˜ìš© ?ì—­**: 3Ã—3 ?ëŠ” 5Ã—5 ì»¤ë„ë¡œëŠ” ?¥ê¸° ?˜ì¡´??long-term dependency) ?¬ì°© ?´ë ¤?€
- ??**ë³µì¡??êµ¬ì¡° ë³µì› ?œê³„**: ?œê³„???°ì´?°ì˜ ê³„ì ˆ?? ?¸ë Œ????ê¸€ë¡œë²Œ ?¨í„´ ?´í•´ ë¶€ì¡?
- ??**?¬ì†Œ ?°ì´??ì²˜ë¦¬**: ê²°ì¸¡ê°’ì´ ë§ì? ë¬´ì—­ ?°ì´??60% ?¬ì†Œ???ì„œ ?±ëŠ¥ ?€??

**?œê³„???ˆì¸¡?ì„œ??ë¬¸ì œ:**
```python
# CNN?€ ê³ ì •??ì»¤ë„ ?¬ê¸°ë¡??¸í•´ ?¤ì–‘??lag ?¨í„´ ?¬ì°© ?´ë ¤?€
# ?? lag=1, lag=3, lag=7???™ì‹œ???™ìŠµ?˜ê¸° ?´ë ¤?€
conv1d = Conv1D(filters=64, kernel_size=3)  # 3ê°œì›” ë²”ìœ„ë§??™ìŠµ
```

#### 2. **Transformer ê¸°ë°˜ ëª¨ë¸???œê³„**

**?¹ì§•:**
- Self-Attention?¼ë¡œ ?„ì²´ ?œí€€??ê°?ê´€ê³??™ìŠµ
- ?¥ê¸° ?˜ì¡´???¬ì°© ê°€??

**?œê³„??**
- ??**ê³„ì‚° ë¹„ìš© ??°œ**: O(nÂ²) ë³µì¡?„ë¡œ ?œí€€??ê¸¸ì´ê°€ ê¸¸ë©´ ë©”ëª¨ë¦??œê°„ ?Œëª¨ ë§‰ë?
  - 100ê°??ˆëª© Ã— 65ê°œì›” = 6,500ê°??œí€€????42,250,000ë²??°ì‚°
- ??**?€?´ìƒ???œí•œ**: ê³„ì‚°??ë¬¸ì œë¡??¤ìš´?˜í”Œë§??„ìš” ???¸ë? ?•ë³´ ?ì‹¤
- ??**?™ìŠµ ë¶ˆì•ˆ?•ì„±**: Layer Normalization??ë¬¸ì œ??ë°œê²¬

**Layer Normalization??ë¬¸ì œ:**
> "We observe unstable optimization using the general block when handling large-scale masks, sometimes incurring gradient exploding. We attribute this training issue to the large ratio of invalid tokens (their values are nearly zero). In this circumstance, layer normalization may magnify useless tokens overwhelmingly, leading to unstable training."

?œê³„???°ì´?°ì—???¬ì†Œ??60%)???’ì„ ?? 0??ê°€ê¹Œìš´ ê°’ë“¤???•ê·œ??ê³¼ì •?ì„œ ?•ë??˜ì–´ ?™ìŠµ??ë¶ˆì•ˆ?•í•´ì§‘ë‹ˆ??

**Residual Connection??ë¬¸ì œ:**
> "Residual learning generally encourages the model to learn high-frequency contents. However, considering most tokens are invalid at the beginning, it is difficult to directly learn high-frequency details without proper low-frequency basis in GAN training, which makes the optimization harder."

ì´ˆê¸° ?¨ê³„?ì„œ ?€ë¶€ë¶„ì˜ ? í°??ë¬´íš¨(invalid)???? ê¸°ë³¸ ë² ì´???†ì´ ê³ ì£¼???¸ë??¬í•­???™ìŠµ?˜ê¸° ?´ë µ?µë‹ˆ??

---

### ë³??„ë¡œ?íŠ¸??? íƒ: XGBoost + Lagged Correlation

?„ì˜ ?œê³„ë¥?ê·¹ë³µ?˜ê¸° ?„í•´ **?„í†µ?ì´ì§€ë§??¨ê³¼?ì¸ ë°©ë²•ë¡?*??? íƒ?ˆìŠµ?ˆë‹¤.

---

## ëª¨ë¸ ì§„í™” ê³¼ì •: 5ê°?ëª¨ë¸???¤í—˜ê³?ê°œì„ 

ë³??„ë¡œ?íŠ¸?ì„œ??ì´?5ê°œì˜ ëª¨ë¸???œì°¨?ìœ¼ë¡?ê°œë°œ?˜ë©° ìµœì ???‘ê·¼ë²•ì„ ?ìƒ‰?ˆìŠµ?ˆë‹¤. ê°?ëª¨ë¸???œê³„ë¥?ë¶„ì„?˜ê³  ê°œì„  ë°©í–¥??ì°¾ì•„ê°€??ê³¼ì •??ê¸°ë¡?©ë‹ˆ??

---

### ?“Š **ëª¨ë¸ 1: ì´ˆê¸° ?‘ê·¼** (0.3493) ??

**êµ¬ì„±:**
- **?¹ì„± ??*: 14ê°?(value ê¸°ë°˜ë§?
- **ê³µí–‰????*: 3,000ê°?
- **?ê?ê³„ìˆ˜ ?„ê³„ê°?*: 0.30
- **Lag ë²”ìœ„**: 1~7ê°œì›”
- **ëª¨ë¸**: XGBoost ?¨ì¼ ëª¨ë¸
- **?˜ì´?¼íŒŒ?¼ë???*: n_estimators=150, max_depth=5, learning_rate=0.08

**?¹ì„± êµ¬ì„±:**
```
???„í–‰ ?ˆëª© (5ê°?: b_t, b_t_1, b_t_2, b_ma3, b_change
??? í–‰ ?ˆëª© (4ê°?: a_t_lag, a_t_lag_1, a_ma3, a_change
??ê´€ê³??¹ì„± (5ê°?: ab_value_ratio, max_corr, best_lag, consistency, corr_stability
```

**?±ëŠ¥:**
- **?ìˆ˜**: 0.3493
- **Baseline ?€ë¹?*: +9.1%
- **?¹ì§•**: ?¨ìˆœ?˜ê³  ?ˆì •?? ê³¼ì ???†ìŒ

**ì²??œë„???¤ê³„ ë°©í–¥:**
1. **?œê³„??ê¸°ë³¸ ?”ì†Œ**: lag, ?´ë™?‰ê· (MA), ë³€?”ìœ¨ ???„í†µ???œê³„???¹ì„± ?¬ìš©
2. **?¨ìˆœ??êµ¬ì¡°**: ë³µì¡?„ë? ??¶° ?ˆì •???•ë³´
3. **?¨ì¼ ëª¨ë¸**: XGBoost ?˜ë‚˜ë¡??œì‘?˜ì—¬ ê¸°ì????•ë³´

---

### ??**ëª¨ë¸ 2: ê³ ê¸‰ ëª¨ë¸** (0.3348) - ?¤íŒ¨

**êµ¬ì„±:**
- **?¹ì„± ??*: 28ê°?(+100% ì¦ê?)
- **ê³µí–‰????*: 3,500ê°?(+17%)
- **ì¶”ê? ?°ì´??*: weight, quantity, trade_frequency, avg_trade_value
- **ëª¨ë¸**: XGBoost (max_depth=6, n_estimators=200)

**ì¶”ê????¹ì„± (14ê°?:**
```
??weight ?¹ì„± (4ê°?: b_weight, b_weight_ma3, a_weight_lag, a_weight_ma3
??quantity ?¹ì„± (2ê°?: b_quantity, a_quantity_lag
??trade frequency ?¹ì„± (2ê°?: b_trade_freq, a_trade_freq
??avg trade value ?¹ì„± (2ê°?: b_avg_value, a_avg_value
??ë³µí•© ?¹ì„± (2ê°?: ab_weight_ratio, ab_quantity_ratio
??ê´€ê³??¹ì„± (2ê°?: hs4_similarity ??
```

**?±ëŠ¥:**
- **?ìˆ˜**: 0.3348
- **ëª¨ë¸ 1 ?€ë¹?*: -4.1% ??
- **ë¬¸ì œ??*: weight/quantity ?°ì´?°ê? ?¸ì´ì¦ˆë¡œ ?‘ìš©, ê³¼ì ??ë°œìƒ

**???¤íŒ¨?ˆëŠ”ê°€?**

| ê°€??| ê²°ê³¼ | ?ì¸ |
|------|------|------|
| "??ë§ì? ?¹ì„± ????ì¢‹ì? ?±ëŠ¥" | ???¤íˆ???€??| ?¸ì´ì¦?ì¦ê? |
| "weight/quantity??? ì˜ë¯? | ???ê????†ìŒ | ?°ì´???ˆì§ˆ ë¬¸ì œ (60% ?¬ì†Œ?? |
| "ë³µì¡??ê´€ê³??¬ì°©" | ??ê³¼ì ??ë°œìƒ | max_depth=6?€ ?ˆë¬´ ê¹ŠìŒ |

ì´ˆê¸° ?‘ê·¼(ëª¨ë¸ 1)?ì„œ ì¢‹ì? ?±ëŠ¥???˜ì™”ê¸??Œë¬¸??"?¹ì„±????ì¶”ê??˜ë©´ ??ì¢‹ì•„ì§€ì§€ ?Šì„ê¹?"?¼ëŠ” ?ê°?¼ë¡œ ?œë„?ˆìœ¼???¤íŒ¨?ˆìŠµ?ˆë‹¤.

**êµ¬ì²´??ë¬¸ì œ??**
1. **weight/quantity???¬ì†Œ??*: 
   - ?„ì²´ ?°ì´?°ì˜ 60%ê°€ 0 ?ëŠ” ê²°ì¸¡ê°?
   - ?ê?ê´€ê³„ê? ?„ë‹Œ **ë¬´ì‘???¸ì´ì¦?*ë¡??‘ìš©
   
2. **Feature Noise ì¦ê?**:
   ```python
   ê¸°ë³¸ ëª¨ë¸: Signal/Noise = 14/0 = ??
   ê³ ê¸‰ ëª¨ë¸: Signal/Noise = 14/14 = 1.0 (50% ?¸ì´ì¦?
   ```

3. **ê³¼ì ??ì§•í›„**:
   - ê²€ì¦??°ì´?°ì—???±ëŠ¥ ?€??
   - ë³µì¡???¹ì„± ì¡°í•©???™ìŠµ ?°ì´?°ì—ë§?ìµœì ??

---

### ? ï¸ **ëª¨ë¸ 3: ?¨ìˆœ??ëª¨ë¸** (ë¯¸í…Œ?¤íŠ¸)

**êµ¬ì„±:**
- **?¹ì„± ??*: 14ê°?(?¸ì´ì¦??œê±°)
- **ê³µí–‰????*: 3,500ê°?(? ì?)
- **?¹ì„± êµ¬ì„±**: ê¸°ë³¸ value ?¹ì„± 10ê°?+ HS4 ? ì‚¬???¬í•¨ ê´€ê³??¹ì„± 4ê°?
- **ëª¨ë¸**: XGBoost (max_depth=5, n_estimators=150)

**ê°œì„  ?„ëµ:**
```diff
- weight/quantity/trade_freq ?¹ì„± 14ê°??œê±° ??
+ HS4 ? ì‚¬??(hs4_similarity) ? ì? ??
+ ê³µí–‰???ì? 3,500ê°œë¡œ ? ì? (ê´€ê³??ìƒ‰ ë²”ìœ„ ?•ë?)
```

**?????‘ê·¼???œë„?ˆëŠ”ê°€?**
1. **?¸ì´ì¦??œê±°**: weight/quantity??ë²„ë¦¬?? HS4 ? ì‚¬?±ì? ?˜ë? ?ˆì„ ê²ƒìœ¼ë¡??ë‹¨
2. **ê´€ê³??ìƒ‰ ?•ë?**: ?ì„ 3,500ê°œë¡œ ?˜ë ¤ ??ë§ì? ?¨í„´ ?ìƒ‰
3. **?ˆì •???Œë³µ**: max_depth=5ë¡?ë³µê?

**ê°€??**
> "ê³ ê¸‰ ëª¨ë¸??ë¬¸ì œ??weight/quantity ?¸ì´ì¦ˆì??? HS4 ? ì‚¬?±ë§Œ ?¨ê¸°ë©?ì´ˆê¸° ?‘ê·¼(0.3493)??ì´ˆê³¼??ê²ƒì´??"

**ê²°ê³¼:**
- **?ìˆ˜**: ë¯¸í…Œ?¤íŠ¸ (?œì¶œ?˜ì? ?ŠìŒ)
- **?´ìœ **: ?¤ìŒ ëª¨ë¸(ì´ˆê³ ê¸???ë¨¼ì? ?œë„

---

### ?ŒâŒ **ëª¨ë¸ 4: ì´ˆê³ ê¸??™ìƒë¸?* (0.293) - ?€?¤íŒ¨

**êµ¬ì„±:**
- **?¹ì„± ??*: 65ê°?(+364% ??°œ)
- **ê³µí–‰????*: 5,000ê°?(+67%)
- **Lag ë²”ìœ„**: 1~12ê°œì›” (ê¸°ì¡´ 1~7?ì„œ ?•ë?)
- **ëª¨ë¸**: XGBoost + LightGBM + CatBoost 3-ëª¨ë¸ ?™ìƒë¸?
- **?™ìƒë¸?ê°€ì¤‘ì¹˜**: 40% + 35% + 25%

**?¹ì„± êµ¬ì„± (65ê°?:**
```
1. ?œê³„???¹ì„± (35ê°?:
   ???´ë™?‰ê· : ma3, ma6, ma12
   ??ë³€?”ìœ¨: change_rate_1m, 3m, 6m
   ??ê°€?ë„: acceleration
   ??ëª¨ë©˜?€: momentum_3m, 6m
   ??ë³€?™ì„±: volatility_3m, 6m, 12m
   ??RSI (Relative Strength Index)
   ??ê¸°í? ê¸°ìˆ ??ì§€??

2. ?ˆëª© ?¹ì„± (20ê°?:
   ???¸ë Œ?? linear_trend, trend_strength
   ??ê³„ì ˆ?? seasonality_score
   ???ˆì •?? stability_score
   ??trading_freq ê´€???¹ì„±

3. ê´€ê³??¹ì„± (10ê°?:
   ??max_corr, recent_corr (ìµœê·¼ 3ê°œì›”), mid_corr (3~6ê°œì›”)
   ??HS4 ? ì‚¬??
   ??ë³µí•© ê´€ê³?ì§€??
```

**?±ëŠ¥:**
- **?ìˆ˜**: 0.293
- **ê¸°ë³¸ ëª¨ë¸ ?€ë¹?*: -16.0% ?ŒâŒ
- **Baseline ?€ë¹?*: -8.5% (ë² ì´?¤ë¼?¸ë³´?¤ë„ ??Œ!)

**???´ë ‡ê²??¤íŒ¨?ˆëŠ”ê°€?**

| ë¬¸ì œ | ?¤ëª… | ?í–¥ |
|------|------|------|
| **ê³¼ì ?©ì˜ ?•ì„** | 65ê°??¹ì„±???™ìŠµ ?°ì´???”ê¸° | ê²€ì¦??±ëŠ¥ ??½ |
| **ë³µì¡????°œ** | 3ê°?ëª¨ë¸ ?™ìƒë¸?+ 65ê°??¹ì„± | ê³„ì‚° ?œê°„ 5ë°?ì¦ê? |
| **Lag ê³¼ë‹¤** | 1~12ê°œì›” ???¥ê¸° ?¨í„´ ê°•ì œ ?™ìŠµ | ?¨ê¸° ? í˜¸ ?œê³¡ |
| **?¸ì´ì¦??¬ë“±??* | weight/quantity ? ì‚¬ ?¹ì„± ?¤ìˆ˜ ?¬í•¨ | Signal/Noise = 20/45 |

**êµ¬ì²´???¤íŒ¨ ?ì¸:**

1. **?˜ë? ?†ëŠ” ?¹ì„± ê³¼ë‹¤:**
   ```python
   momentum_6m, acceleration, RSI ??ë¬´ì—­ ?°ì´?°ì? ë¬´ê?
   (ì£¼ì‹ ê¸°ìˆ ??ì§€?œë? ë¬´ì—­ ?°ì´?°ì— ë¬´ë¦¬?˜ê²Œ ?ìš©)
   ```

2. **?™ìƒë¸”ì˜ ??š¨ê³?**
   - 3ê°?ëª¨ë¸??ëª¨ë‘ ê°™ì? ?¸ì´ì¦ˆë? ?™ìŠµ
   - ?™ìƒë¸?= ê³¼ì ??Ã— 3
   - ?¤ì–‘???†ì´ ë³µì¡?„ë§Œ ì¦ê?

3. **Lag ë²”ìœ„ ê³¼ë„ ?•ë?:**
   - 1~7ê°œì›”: ?¨ê¸° ê³µí–‰???¬ì°© ??
   - 1~12ê°œì›”: ?¥ê¸° ?¸ë Œ??ê°•ì œ ?™ìŠµ ??
   - 12ê°œì›” lag??ê³„ì ˆ?±ì´ì§€ ê³µí–‰?±ì´ ?„ë‹˜

**êµí›ˆ:**
> "ë³µì¡?????±ëŠ¥. ì£¼ì‹ ë¶„ì„ ê¸°ë²•??ë¬´ì—­ ?°ì´?°ì— ?ìš©?˜ë©´ ???œë‹¤."

---

### ?¯ **ëª¨ë¸ 5: ?¤ìš© ëª¨ë¸** (?ˆìƒ 0.37-0.40)

**êµ¬ì„±:**
- **?¹ì„± ??*: 14ê°?(ê¸°ë³¸ ëª¨ë¸ ?Œê?)
- **ê³µí–‰????*: 4,000ê°?(+33%)
- **?ê?ê³„ìˆ˜ ?„ê³„ê°?*: 0.28 (0.30 ??0.28 ?„í™”)
- **Lag ë²”ìœ„**: 1~7ê°œì›” (ê²€ì¦ëœ ë²”ìœ„)
- **ëª¨ë¸**: XGBoost + LightGBM (60:40 ?™ìƒë¸?

**?˜ì´?¼íŒŒ?¼ë???ìµœì ??**
```python
XGBoost:
  n_estimators=250 (150 ??250)
  max_depth=5 (? ì?)
  learning_rate=0.06 (0.08 ??0.06, ???ˆì •??
  min_child_weight=6 (ê³¼ì ??ë°©ì? ê°•í™”)
  reg_alpha=0.6, reg_lambda=1.2 (?•ê·œ??ê°•í™”)

LightGBM:
  n_estimators=250
  max_depth=6
  learning_rate=0.06
  min_child_weight=25 (ê³¼ì ??ë°©ì? ê·¹ë???
```

**????ë°©ë²•??? íƒ?ˆëŠ”ê°€?**

| ê²°ì • | ?´ìœ  | ê·¼ê±° |
|------|------|------|
| **14ê°??¹ì„± ?Œê?** | ê²€ì¦ëœ ?¹ì„±ë§??¬ìš© | ì´ˆê¸° ?‘ê·¼?ì„œ ?¨ê³¼ ?…ì¦ |
| **4,000ê°???* | ê´€ê³??ìƒ‰ ë²”ìœ„ ?•ë? | ?„ê³„ê°?0.28ë¡??„í™” ????ë§ì? ?½í•œ ?ê?ê´€ê³??¬ì°© |
| **2-ëª¨ë¸ ?™ìƒë¸?* | ?¤ì–‘???•ë³´ | XGBoost(ê¹Šì´?? + LightGBM(ê¹Šì´?? ì¡°í•© |
| **?•ê·œ??ê°•í™”** | ê³¼ì ??ë°©ì? | reg_alpha=0.6, min_child_weight ì¦ê? |

**ê°œì„  ?„ëµ:**

1. **ë³µì¡??ê°ì†Œ + ê´€ê³??ìƒ‰ ì¦ê?:**
   ```
   ëª¨ë¸ 4: 65ê°??¹ì„± Ã— 5,000??= 325,000 ì°¨ì› ??
   ëª¨ë¸ 5: 14ê°??¹ì„± Ã— 4,000??= 56,000 ì°¨ì› ??
   ```
   ??82% ë³µì¡??ê°ì†Œ, 33% ê´€ê³??ìƒ‰ ì¦ê?

2. **?™ìƒë¸?ìµœì†Œ??**
   - 3-ëª¨ë¸ ?™ìƒë¸?(ëª¨ë¸ 4) ??ê³¼ì ??ì¦í­
   - 2-ëª¨ë¸ ?™ìƒë¸?(ëª¨ë¸ 5) ???¤ì–‘???•ë³´

3. **?„ê³„ê°??„í™”???¨ê³¼:**
   ```python
   0.30 ?„ê³„ê°? ê°•í•œ ?ê?ê´€ê³?3,000??
   0.28 ?„ê³„ê°? ì¤‘ê°„ ?ê?ê´€ê³?4,000??(+33%)
   ```
   ???½í•œ ê³µí–‰?±ë„ ?¬ì°©?˜ì—¬ Recall ?¥ìƒ

**?ˆìƒ ?±ëŠ¥:**
- **ëª©í‘œ**: 0.37-0.40
- **ê·¼ê±°**:
  - ê¸°ë³¸ ëª¨ë¸(0.3493) + ê´€ê³??ìƒ‰ ?•ë?(+33%) + ìµœì ??
  - ê³¼ì ??ë°©ì?ë¡??ˆì •???•ë³´

---

### ?“Š **5ê°?ëª¨ë¸ ë¹„êµ ?”ì•½**

| ëª¨ë¸ | ?¹ì„± | ??| ëª¨ë¸ ??| ?ìˆ˜ | ì°¨ì´ | ?µì‹¬ ë¬¸ì œ/ê°œì„  |
|------|------|------|---------|------|------|----------------|
| **Baseline** | - | - | - | 0.3201 | - | - |
| **ëª¨ë¸ 1 (ì´ˆê¸°)** ??| 14 | 3,000 | 1 | **0.3493** | +9.1% | ì²??œë„ ê²°ê³¼ |
| **ëª¨ë¸ 2 (ê³ ê¸‰)** ??| 28 | 3,500 | 1 | 0.3348 | -4.1% | weight/quantity ?¸ì´ì¦?|
| **ëª¨ë¸ 3 (?¨ìˆœ??** ? ï¸ | 14 | 3,500 | 1 | ë¯¸í…Œ?¤íŠ¸ | ? | ?¸ì´ì¦??œê±° ?œë„ |
| **ëª¨ë¸ 4 (ì´ˆê³ ê¸?** ?ŒâŒ | 65 | 5,000 | 3 | 0.293 | -16.0% | ê·¹ì‹¬??ê³¼ì ??|
| **ëª¨ë¸ 5 (?¤ìš©)** ?¯ | 14 | 4,000 | 2 | 0.37~0.40? | +5~14% | ?¨ìˆœ??+ ìµœì ??|

---

### ?’¡ **?µì‹¬ êµí›ˆ**

#### 1. **ë³µì¡??ëª¨ë¸ ??ì¢‹ì? ?±ëŠ¥**

```
14ê°??¹ì„± (ì²??œë„) ??0.3493 ??
28ê°??¹ì„± (ëª¨ë¸ 2) ??0.3348 ??
65ê°??¹ì„± (ëª¨ë¸ 4) ??0.293 ?ŒâŒ
```

**ê²°ë¡ :** "Feature Engineering > Feature Quantity"

#### 2. **?°ì´???ˆì§ˆ > ?°ì´????*

```
Valueë§??¬ìš© (14ê°? ???ˆì •??? í˜¸
Weight/Quantity ì¶”ê? (28ê°? ???¸ì´ì¦?ì¦ê? (60% ?¬ì†Œ??
```

**ê²°ë¡ :** "ì¢‹ì? ?¹ì„± 14ê°?>> ?˜ìœ ?¹ì„± 65ê°?

#### 3. **?™ìƒë¸”ì˜ ??„¤**

```
3-ëª¨ë¸ ?™ìƒë¸?(ëª¨ë¸ 4) ??ê³¼ì ??Ã— 3
2-ëª¨ë¸ ?™ìƒë¸?(ëª¨ë¸ 5) ???¤ì–‘???•ë³´
```

**ê²°ë¡ :** "?™ìƒë¸”ì? ?¤ì–‘?±ì´ ?ˆì„ ?Œë§Œ ?¨ê³¼??

#### 4. **?„ë©”??ì§€?ì˜ ì¤‘ìš”??*

```
ì£¼ì‹ ê¸°ìˆ ??ì§€??(RSI, Momentum) ??ë¬´ì—­ ?°ì´?°ì— ë¬´ì˜ë¯???
?œê³„???µì‹¬ ?”ì†Œ (Lag, MA, Change) ???¨ê³¼????
```

**ê²°ë¡ :** "ë²”ìš© ê¸°ë²•ë³´ë‹¤ ?„ë©”???¹í™” ?¤ê³„ê°€ ì¤‘ìš”"

#### 5. **Occam's Razor ê²€ì¦?*

```
"ê°€???¨ìˆœ???¤ëª…??ë³´í†µ ?³ë‹¤"
??14ê°??¹ì„± + XGBoost = ê°€??ì¢‹ì? ?±ëŠ¥
```

**ìµœì¢… ?„ëµ:** ë³µì¡????+ ê´€ê³??ìƒ‰ ??+ ìµœì ??

---

## ì½”ë“œ êµ¬í˜„

### ?°ì´?°ì…‹ êµ¬ì„±ê³??„ì²˜ë¦?

```python
import pandas as pd
import numpy as np
from sklearn.linear_model import LinearRegression
from tqdm import tqdm
from joblib import Parallel, delayed
from xgboost import XGBRegressor
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# ê³µí†µ ?¨ìˆ˜: safe_corr
def safe_corr(x, y):
    """?ˆì „???ê?ê³„ìˆ˜ ê³„ì‚° (?œì??¸ì°¨ 0??ê²½ìš° ì²˜ë¦¬)"""
    if np.std(x) == 0 or np.std(y) == 0:
        return 0.0
    return float(np.corrcoef(x, y)[0, 1])

# ?°ì´??ë¡œë“œ
train = pd.read_csv('./train.csv')

# item_idë³? ?„ì›”ë³?value ?©ê³„ë¥??¼ë´‡ ?Œì´ë¸”ë¡œ ë³€??
pivot_value = train.groupby(['item_id', 'year', 'month'])['value'].sum().reset_index()
pivot_value['year_month'] = pivot_value['year'].astype(str) + '-' + pivot_value['month'].astype(str).str.zfill(2)
pivot_value = pivot_value.pivot(index='item_id', columns='year_month', values='value').fillna(0)

print(f"?°ì´???•íƒœ: {pivot_value.shape}")
print(f"?ˆëª© ?? {len(pivot_value)}")
print(f"???? {len(pivot_value.columns)}")
```

**?„ì²˜ë¦?ê³¼ì •:**
1. CSV ?Œì¼?ì„œ ?ˆëª©ë³??”ë³„ ë¬´ì—­ ?°ì´??value, weight, quantity ?? ë¡œë“œ
2. ?ˆëª©ë³??”ë³„ ?¼ë´‡ ?Œì´ë¸??ì„±?˜ì—¬ ?œê³„???•íƒœë¡?ë³€??
3. ê²°ì¸¡ê°’ì„ 0?¼ë¡œ ì²˜ë¦¬?˜ì—¬ ?°ì´???¼ê????•ë³´
4. 2020??1??~ 2025??4?”ê¹Œì§€ ì´?65ê°œì›”???œê³„???°ì´??êµ¬ì„±

**?°ì´???¹ì§•:**
- 100ê°??ˆëª© Ã— 65ê°œì›” = 6,500ê°??œê³„???¬ì¸??
- ??60%???¬ì†Œ??(sparse data) ì¡´ì¬
- value ê¸°ë°˜ ë¶„ì„??ê°€??? ë¢°???’ìŒ (weight, quantity???¸ì´ì¦?ë§ìŒ)

---

### ê³µí–‰???ì? ëª¨ë¸: Lagged Correlation ê¸°ë°˜

```python
def find_comovement_pairs(pivot, max_lag=7, min_nonzero=8, corr_threshold=0.30, n_jobs=-1):
    """
    Lagged correlation???´ìš©??ê³µí–‰?????ìƒ‰
    
    Args:
        pivot: ?ˆëª©ë³??œê³„???°ì´??(item_id Ã— months)
        max_lag: ìµœë? ì§€???œê°„ (1~7ê°œì›”)
        min_nonzero: ìµœì†Œ ë¹„ì˜ ê°?ê°œìˆ˜ (?¬ì†Œ???„í„°ë§?
        corr_threshold: ?ê?ê³„ìˆ˜ ?„ê³„ê°?(0.30)
        n_jobs: ë³‘ë ¬ ì²˜ë¦¬ ì½”ì–´ ??
    
    Returns:
        DataFrame: ê³µí–‰????ëª©ë¡ (leading_item_id, following_item_id, best_lag, max_corr)
    """
    items = pivot.index.to_list()
    months = pivot.columns.to_list()
    n_months = len(months)

    def process_pair(leader, follower):
        """?¨ì¼ ??ì²˜ë¦¬ ?¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬??"""
        x = pivot.loc[leader].values.astype(float)
        y = pivot.loc[follower].values.astype(float)
        
        # ?¬ì†Œ???„í„°ë§? ë¹„ì˜ ê°’ì´ ì¶©ë¶„?œì? ?•ì¸
        if np.count_nonzero(x) < min_nonzero or np.count_nonzero(y) < min_nonzero:
            return None
        
        best_lag = None
        best_corr = 0.0
        second_best_corr = 0.0
        
        # ê°?lag???€???ê?ê³„ìˆ˜ ê³„ì‚°
        for lag in range(1, max_lag + 1):
            if n_months <= lag:
                continue
            
            # A[t-lag]?€ B[t]???ê?ê´€ê³?(?µì‹¬!)
            corr = safe_corr(x[:-lag], y[lag:])
            
            if abs(corr) > abs(best_corr):
                second_best_corr = best_corr
                best_corr = corr
                best_lag = lag
            elif abs(corr) > abs(second_best_corr):
                second_best_corr = corr
        
        # ?„ê³„ê°??´ìƒ?´ë©´ ê³µí–‰???ìœ¼ë¡??ì •
        if best_lag is not None and abs(best_corr) >= corr_threshold:
            # ìµœê·¼ ?ê?ê³„ìˆ˜ ê²€ì¦?(?¼ê????•ì¸)
            recent_corr = 0.0
            if n_months > best_lag + 6:
                recent_x = x[-(6+best_lag):-best_lag]
                recent_y = y[-6:]
                recent_corr = safe_corr(recent_x, recent_y)
            
            # ?¼ê????ìˆ˜ ê³„ì‚°
            consistency = 1.0
            if abs(recent_corr) > 0.2 and np.sign(best_corr) == np.sign(recent_corr):
                consistency = 1.2
            
            return {
                "leading_item_id": leader,
                "following_item_id": follower,
                "best_lag": best_lag,
                "max_corr": best_corr,
                "recent_corr": recent_corr,
                "consistency": consistency,
                "corr_stability": abs(best_corr - second_best_corr),
            }
        
        return None

    # ë³‘ë ¬ ì²˜ë¦¬ë¡?ëª¨ë“  ???ìƒ‰
    results = Parallel(n_jobs=n_jobs)(
        delayed(process_pair)(leader, follower)
        for leader in tqdm(items, desc="Finding comovement pairs")
        for follower in items if leader != follower
    )

    # ê²°ê³¼ë¥?DataFrame?¼ë¡œ ë³€??
    pairs = pd.DataFrame([res for res in results if res is not None])
    
    if len(pairs) > 0:
        # ?ê?ê³„ìˆ˜ ?ˆë?ê°?ê¸°ì??¼ë¡œ ?•ë ¬
        pairs['score'] = pairs['max_corr'].abs() * pairs['consistency']
        pairs = pairs.sort_values('score', ascending=False)
        pairs = pairs.drop('score', axis=1)
    
    return pairs

# ê³µí–‰?????ìƒ‰ ?¤í–‰
pairs = find_comovement_pairs(
    pivot_value,
    max_lag=7,
    min_nonzero=8,
    corr_threshold=0.30
)

print(f"?ìƒ‰??ê³µí–‰?±ìŒ ?? {len(pairs):,}ê°?)
print(f"\nLag ë¶„í¬:")
print(pairs['best_lag'].value_counts().sort_index())
print(f"\n?ê?ê³„ìˆ˜ ?µê³„:")
print(pairs['max_corr'].describe())

# ?ìœ„ 3,000ê°?? íƒ
if len(pairs) > 3000:
    pairs = pairs.head(3000)
    print(f"\n?ìœ„ 3,000ê°????¬ìš©")
```

**Lagged Correlation ? íƒ ?´ìœ :**

1. **?œê³„???¸ê³¼ê´€ê³??¬ì°©**: ? í–‰ ?ˆëª© A[t-lag]ê°€ ?„í–‰ ?ˆëª© B[t]??ë¯¸ì¹˜???í–¥???•ëŸ‰??
2. **?¨ìˆœ?˜ì?ë§??¨ê³¼??*: ë³µì¡???¥ëŸ¬??ëª¨ë¸ ?†ì´???œê°„ ì§€???¨í„´??ëª…í™•??ë°œê²¬
3. **?´ì„ ê°€?¥ì„±**: ?ê?ê³„ìˆ˜?€ lag ê°’ìœ¼ë¡?ì§ê??ì¸ ?´í•´ ê°€??
4. **ê³„ì‚° ?¨ìœ¨??*: 100ê°??ˆëª©???€??ë¹ ë¥¸ ?ìƒ‰ ê°€??

**?Œë¼ë¯¸í„° ?¤ì •:**
- `max_lag=7`: ìµœë? 7ê°œì›”ê¹Œì? ì§€???œê°„ ?ìƒ‰ (ë¬´ì—­ ?°ì´???¹ì„±???¥ê¸° ì§€?°ì? ?œë¬¼??
- `min_nonzero=8`: ìµœì†Œ 8ê°œì›” ?´ìƒ??ê±°ë˜ ?´ë ¥???ˆì–´??? ë¢°???ˆëŠ” ?ê?ê´€ê³?ê³„ì‚°
- `corr_threshold=0.30`: ?ê?ê³„ìˆ˜ ?ˆë?ê°?0.30 ?´ìƒ??? ì˜ë¯¸í•œ ê³µí–‰?±ìœ¼ë¡??ë‹¨

---

### ?ˆì¸¡ ëª¨ë¸: XGBoost Regressor

```python
class XGBRegressor:
    def __init__(self):
        self.model = XGBRegressor(
            n_estimators=150,       # ?¸ë¦¬ ê°œìˆ˜
            max_depth=5,            # ?¸ë¦¬ ê¹Šì´
            learning_rate=0.08,     # ?™ìŠµë¥?
            subsample=0.85,         # ?˜í”Œë§?ë¹„ìœ¨
            colsample_bytree=0.85,  # ?¹ì„± ?˜í”Œë§?ë¹„ìœ¨
            min_child_weight=5,     # ìµœì†Œ ?ì‹ ê°€ì¤‘ì¹˜
            gamma=0.2,              # ë¶„í•  ìµœì†Œ ?ì‹¤ ê°ì†Œ
            reg_alpha=0.5,          # L1 ?•ê·œ??
            reg_lambda=1.0,         # L2 ?•ê·œ??
            random_state=42
        )
```

**XGBoost ? íƒ ?´ìœ :**

1. **Gradient Boosting??ê°•ë ¥??*: ?½í•œ ?™ìŠµê¸??¸ë¦¬)ë¥??œì°¨?ìœ¼ë¡?ê²°í•©?˜ì—¬ ê°•ë ¥???ˆì¸¡ ëª¨ë¸ ?ì„±
2. **ë¹„ì„ ??ê´€ê³??¬ì°©**: ? í–‰-?„í–‰ ?ˆëª© ê°?ë³µì¡??ë¹„ì„ ???¨í„´ ?™ìŠµ ê°€??
3. **?¹ì„± ì¤‘ìš”??ë¶„ì„**: Feature importanceë¡??´ë–¤ ?¹ì„±???ˆì¸¡??ì¤‘ìš”?œì? ?´ì„ ê°€??
4. **ê³¼ì ??ë°©ì?**: Regularization(L1/L2)ê³?Early stopping?¼ë¡œ ?¼ë°˜???±ëŠ¥ ?•ë³´
5. **ë¹ ë¥¸ ?™ìŠµ ?ë„**: ë³‘ë ¬ ì²˜ë¦¬ ë°?ìµœì ?”ë¡œ ?€ê·œëª¨ ?°ì´?°ì—?œë„ ?¨ìœ¨??

**?˜ì´?¼íŒŒ?¼ë????¤ì • ê·¼ê±°:**
- `n_estimators=150`: ì¶©ë¶„???™ìŠµ + ê³¼ì ??ë°©ì? ê· í˜•??
- `max_depth=5`: ê¹Šì? ?¸ë¦¬??ê³¼ì ???„í—˜, 5?¨ê³„ê°€ ?ì ˆ
- `learning_rate=0.08`: ??? ?™ìŠµë¥ ë¡œ ?ˆì •???˜ë ´
- `subsample/colsample_bytree=0.85`: 85% ?˜í”Œë§ìœ¼ë¡?robustness ?•ë³´
- `min_child_weight=5`: ?ˆë¬´ ?‘ì? ë¶„í•  ë°©ì?
- `reg_alpha=0.5, reg_lambda=1.0`: L1/L2 ?•ê·œ?”ë¡œ ê°€ì¤‘ì¹˜ ?œì–´

---

### ?™ìŠµ ?°ì´???ì„±: 14ê°??¹ì„±

```python
def create_training_data(pivot_value, pairs):
    """
    ê³µí–‰???ì— ?€???™ìŠµ ?°ì´???ì„± (14ê°??¹ì„±)
    
    Args:
        pivot_value: ?ˆëª©ë³??œê³„???°ì´??
        pairs: ê³µí–‰????DataFrame
    
    Returns:
        DataFrame: ?™ìŠµ ?°ì´??(features + target)
    """
    samples = []
    months = pivot_value.columns.to_list()
    n_months = len(months)
    
    for row in tqdm(pairs.itertuples(index=False), desc="Creating training data"):
        leader = row.leading_item_id
        follower = row.following_item_id
        lag = int(row.best_lag)
        corr = float(row.max_corr)
        consistency = getattr(row, 'consistency', 1.0)
        
        if (leader not in pivot_value.index or follower not in pivot_value.index):
            continue
        
        # ?œê³„??ì¶”ì¶œ
        b_value = pivot_value.loc[follower].values.astype(float)  # ?„í–‰ ?ˆëª©
        a_value = pivot_value.loc[leader].values.astype(float)    # ? í–‰ ?ˆëª©
        
        # ê°??œì  t???€???˜í”Œ ?ì„± (t+1???ˆì¸¡)
        for t in range(lag + 3, n_months - 1):
            # ========================================
            # ?„í–‰ ?ˆëª© ?¹ì„± (5ê°?
            # ========================================
            b_t = b_value[t]          # ?„ì¬ ê°?
            b_t_1 = b_value[t - 1]    # 1ê°œì›” ??
            b_t_2 = b_value[t - 2]    # 2ê°œì›” ??
            b_ma3 = np.mean(b_value[max(0, t-2):t+1])  # 3ê°œì›” ?´ë™?‰ê· 
            b_change = (b_t - b_t_1) / (b_t_1 + 1)     # ë³€?”ìœ¨
            
            # ========================================
            # ? í–‰ ?ˆëª© ?¹ì„± (4ê°?
            # ========================================
            a_t_lag = a_value[t - lag]              # lag ?œì  ê°?(?µì‹¬!)
            a_t_lag_1 = a_value[t - lag - 1] if (t - lag - 1) >= 0 else 0.0
            a_ma3 = np.mean(a_value[max(0, t-lag-2):t-lag+1])  # ?´ë™?‰ê· 
            a_change = (a_t_lag - a_t_lag_1) / (a_t_lag_1 + 1) # ë³€?”ìœ¨
            
            # ========================================
            # ê´€ê³??¹ì„± (5ê°?
            # ========================================
            ab_value_ratio = b_t / (a_t_lag + 1)  # ê°?ë¹„ìœ¨
            # max_corr, best_lag, consistency, corr_stability?????•ë³´?ì„œ ê°€?¸ì˜´
            
            # ========================================
            # ?€ê²? ?¤ìŒ ???„í–‰ ?ˆëª© ê°?
            # ========================================
            target = b_value[t + 1]
            
            samples.append({
                # ?„í–‰ ?ˆëª© ?¹ì„±
                'b_t': b_t,
                'b_t_1': b_t_1,
                'b_t_2': b_t_2,
                'b_ma3': b_ma3,
                'b_change': b_change,
                
                # ? í–‰ ?ˆëª© ?¹ì„±
                'a_t_lag': a_t_lag,
                'a_t_lag_1': a_t_lag_1,
                'a_ma3': a_ma3,
                'a_change': a_change,
                
                # ê´€ê³??¹ì„±
                'ab_value_ratio': ab_value_ratio,
                'max_corr': corr,
                'best_lag': float(lag),
                'consistency': consistency,
                'corr_stability': float(row.corr_stability),
                
                # ?€ê²?
                'target': target
            })
    
    df_train = pd.DataFrame(samples)
    return df_train

# ?™ìŠµ ?°ì´???ì„±
df_train = create_training_data(pivot_value, pairs)

print(f"?ì„±???™ìŠµ ?˜í”Œ ?? {len(df_train):,}ê°?)
print(f"?¹ì„± ê°œìˆ˜: {len(df_train.columns) - 1}ê°?)  # target ?œì™¸
print(f"\n?€ê²??µê³„:")
print(df_train['target'].describe())
```

**?¹ì„± ?¤ê³„ ì² í•™:**

**1. ?„í–‰ ?ˆëª© ?œê³„???¹ì„± (5ê°?**
- `b_t, b_t_1, b_t_2`: ìµœê·¼ 3ê°œì›” ê°’ìœ¼ë¡??¨ê¸° ?¸ë Œ???Œì•…
- `b_ma3`: 3ê°œì›” ?´ë™?‰ê· ?¼ë¡œ ?¸ì´ì¦??œê±° ë°?ì¤‘ê¸° ?¨í„´ ?¬ì°©
- `b_change`: ë³€?”ìœ¨ë¡??ìŠ¹/?˜ë½ momentum ë°˜ì˜

**2. ? í–‰ ?ˆëª© ?œê³„???¹ì„± (4ê°?**
- `a_t_lag, a_t_lag_1`: lag ?œì ??? í–‰ ?ˆëª© ê°?(?µì‹¬ ?¸ê³¼ ?•ë³´)
- `a_ma3`: ? í–‰ ?ˆëª©???ˆì •?ì¸ ?¸ë Œ??
- `a_change`: ? í–‰ ?ˆëª©??momentum

**3. ê´€ê³??¹ì„± (5ê°?**
- `ab_value_ratio`: ???ˆëª© ê°?ê·œëª¨ ë¹„ìœ¨
- `max_corr`: ê³µí–‰??ê°•ë„
- `best_lag`: ìµœì  ?œê°„ ì§€??
- `consistency`: ?ê?ê´€ê³??¼ê???
- `corr_stability`: ?ê?ê´€ê³??ˆì •??

**?¹ì„± ê°œìˆ˜ë¥?14ê°œë¡œ ?œí•œ???´ìœ :**
- ??**?¨ìˆœ?¨ì´ ìµœê³ **: 28ê°? 65ê°œë¡œ ?˜ë¦´?˜ë¡ ?±ëŠ¥ ?€??(ê³¼ì ??
- ??**?„ë©”??ì§€??ë°˜ì˜**: ?œê³„??ë¶„ì„???µì‹¬ ?”ì†Œë§?? ë³„
- ??**?´ì„ ê°€?¥ì„±**: ê°??¹ì„±???˜ë?ê°€ ëª…í™•??
- ??**?¼ë°˜???±ëŠ¥**: ê²€ì¦??°ì´?°ì—?œë„ ?ˆì •??

---

### ?ˆì¸¡ ?Œì´?„ë¼??

```python
def predict(pivot_value, pairs, model, feature_cols):
    """
    ê³µí–‰???ì— ?€???¤ìŒ ??ê°??ˆì¸¡
    
    Args:
        pivot_value: ?ˆëª©ë³??œê³„???°ì´??
        pairs: ê³µí–‰????DataFrame
        model: ?™ìŠµ??XGBoost ëª¨ë¸
        feature_cols: ?¹ì„± ì»¬ëŸ¼ ë¦¬ìŠ¤??
    
    Returns:
        DataFrame: ?ˆì¸¡ ê²°ê³¼ (leading_item_id, following_item_id, target)
    """
    predictions = []
    months = pivot_value.columns.to_list()
    t_last = len(months) - 1  # ë§ˆì?ë§??œì 
    
    for row in tqdm(pairs.itertuples(index=False), desc="Predicting"):
        leader = row.leading_item_id
        follower = row.following_item_id
        lag = int(row.best_lag)
        corr = float(row.max_corr)
        consistency = getattr(row, 'consistency', 1.0)
        
        # ?ˆëª© ì¡´ì¬ ?¬ë? ?•ì¸
        if (leader not in pivot_value.index or follower not in pivot_value.index):
            continue
        
        # ?œê³„??ì¶”ì¶œ
        b = pivot_value.loc[follower].values.astype(float)
        a = pivot_value.loc[leader].values.astype(float)
        
        # ========================================
        # ?¹ì„± ê³„ì‚° (ë§ˆì?ë§??œì  ê¸°ì?)
        # ========================================
        t = t_last
        
        # ?„í–‰ ?ˆëª© ?¹ì„±
        b_t = b[t]
        b_t_1 = b[t-1]
        b_t_2 = b[t-2]
        b_ma3 = np.mean(b[max(0, t-2):t+1])
        b_change = (b_t - b_t_1) / (b_t_1 + 1) if b_t_1 > 0 else 0
        
        # ? í–‰ ?ˆëª© ?¹ì„±
        if t - lag < 0:
            continue  # lagê°€ ?ˆë¬´ ?¬ë©´ ?¤í‚µ
        
        a_t_lag = a[t - lag]
        a_t_lag_1 = a[t - lag - 1] if (t - lag - 1) >= 0 else 0.0
        a_ma3 = np.mean(a[max(0, t-lag-2):t-lag+1])
        a_change = (a_t_lag - a_t_lag_1) / (a_t_lag_1 + 1) if a_t_lag_1 > 0 else 0
        
        # ê´€ê³??¹ì„±
        ab_value_ratio = b_t / (a_t_lag + 1)
        
        # ?¹ì„± ë²¡í„° êµ¬ì„±
        features = {
            'b_t': b_t,
            'b_t_1': b_t_1,
            'b_t_2': b_t_2,
            'b_ma3': b_ma3,
            'b_change': b_change,
            'a_t_lag': a_t_lag,
            'a_t_lag_1': a_t_lag_1,
            'a_ma3': a_ma3,
            'a_change': a_change,
            'ab_value_ratio': ab_value_ratio,
            'max_corr': corr,
            'best_lag': float(lag),
            'consistency': consistency,
            'corr_stability': float(row.corr_stability)
        }
        
        # ========================================
        # ?ˆì¸¡
        # ========================================
        X = np.array([[features[col] for col in feature_cols]])
        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)  # ?ˆì „ ì²˜ë¦¬
        
        y_pred = model.predict(X)[0]
        
        # ========================================
        # ?„ì²˜ë¦?(ë¹„ì •??ê°??œê±°)
        # ========================================
        
        # 1. ?Œìˆ˜ ?œê±°
        if y_pred < 0:
            y_pred = b_ma3  # ?´ë™?‰ê· ?¼ë¡œ ?€ì²?
        
        # 2. ê·¹ë‹¨ê°??œí•œ
        recent_max = np.max(b[-6:])  # ìµœê·¼ 6ê°œì›” ìµœë?ê°?
        recent_min = np.min(b[-6:])  # ìµœê·¼ 6ê°œì›” ìµœì†Œê°?
        
        if y_pred > recent_max * 2.0:
            # ìµœë?ê°’ì˜ 2ë°°ë? ì´ˆê³¼?˜ë©´ 1.3ë°°ë¡œ ?œí•œ
            y_pred = recent_max * 1.3
        elif y_pred < recent_min * 0.3 and recent_min > 0:
            # ìµœì†Œê°’ì˜ 30% ë¯¸ë§Œ?´ë©´ 70%ë¡??œí•œ
            y_pred = recent_min * 0.7
        
        # 3. ?´ë™?‰ê· ê³?ì°¨ì´ê°€ ?ˆë¬´ ?¬ë©´ ë³´ì •
        if abs(y_pred - b_ma3) > b_ma3 * 2:
            y_pred = 0.6 * y_pred + 0.4 * b_ma3  # ê°€ì¤??‰ê· 
        
        # 4. ?•ìˆ˜ ë³€??(ë¬´ì—­?‰ì? ?•ìˆ˜)
        y_pred = int(round(y_pred))
        
        predictions.append({
            'leading_item_id': leader,
            'following_item_id': follower,
            'target': max(0, y_pred)  # ?Œìˆ˜ ë°©ì?
        })
    
    return pd.DataFrame(predictions)

# ?ˆì¸¡ ?¤í–‰
predictions = predict(pivot_value, pairs, model, feature_cols)
predictions.to_csv('submission_improved.csv', index=False)

print(f"???ˆì¸¡ ?„ë£Œ: {len(predictions)}ê°?)
print(f"\n?ˆì¸¡ê°??µê³„:")
print(predictions['target'].describe())
```

**?ˆì¸¡ ?„ì²˜ë¦??„ëµ:**

1. **?Œìˆ˜ ?œê±°**: ë¬´ì—­?‰ì? ?Œìˆ˜?????†ìœ¼ë¯€ë¡??´ë™?‰ê· ?¼ë¡œ ?€ì²?
2. **ê·¹ë‹¨ê°??œí•œ**: 
   - ìµœê·¼ 6ê°œì›” ìµœë?ê°’ì˜ 2ë°?ì´ˆê³¼ ??1.3ë°°ë¡œ ?œí•œ
   - ìµœê·¼ 6ê°œì›” ìµœì†Œê°’ì˜ 30% ë¯¸ë§Œ ??70%ë¡??œí•œ
3. **?´ë™?‰ê·  ë³´ì •**: ?´ë™?‰ê· ê³?ì°¨ì´ê°€ 2ë°??´ìƒ?´ë©´ ê°€ì¤??‰ê·  ?ìš©
4. **?•ìˆ˜ ë³€??*: ë¬´ì—­?‰ì? ?•ìˆ˜?¬ì•¼ ?˜ë?ë¡?ë°˜ì˜¬ë¦?
        leader = pair['leading_item_id']
        follower = pair['following_item_id']
        lag = pair['best_lag']
        
        # ?œê³„??ì¶”ì¶œ
        b = pivot_value.loc[follower].values
        a = pivot_value.loc[leader].values
        
        # ?¹ì„± ê³„ì‚° (ë§ˆì?ë§??œì  ê¸°ì?)
        t = t_last
        features = {
            'b_t': b[t],
            'b_t_1': b[t-1],
            'b_t_2': b[t-2],
            'b_ma3': np.mean(b[max(0, t-2):t+1]),
            'b_change': (b[t] - b[t-1]) / (b[t-1] + 1),
            'a_t_lag': a[t - lag],
            'a_t_lag_1': a[t - lag - 1],
            'a_ma3': np.mean(a[max(0, t-lag-2):t-lag+1]),
            'a_change': (a[t-lag] - a[t-lag-1]) / (a[t-lag-1] + 1),
            'ab_value_ratio': b[t] / (a[t-lag] + 1),
            'max_corr': pair['max_corr'],
            'best_lag': lag,
            'consistency': 1.0,
            'corr_stability': 0.1
        }
        
        # ?ˆì¸¡
        X = np.array([[features[col] for col in feature_cols]])
        y_pred = model.predict(X)[0]
        
        # ?„ì²˜ë¦?(ë¹„ì •??ê°??œê±°)
        if y_pred < 0:
            y_pred = features['b_ma3']
        
        # ê·¹ë‹¨ê°??œí•œ
        recent_max = np.max(b[-6:])
        if y_pred > recent_max * 2.0:
            y_pred = recent_max * 1.3
        
        predictions.append({
            'leading_item_id': leader,
            'following_item_id': follower,
            'target': int(round(y_pred))
        })
    
    return pd.DataFrame(predictions)
```

**?ˆì¸¡ ?„ì²˜ë¦??„ëµ:**

1. **?Œìˆ˜ ?œê±°**: ë¬´ì—­?‰ì? ?Œìˆ˜?????†ìœ¼ë¯€ë¡??´ë™?‰ê· ?¼ë¡œ ?€ì²?
2. **ê·¹ë‹¨ê°??œí•œ**: ìµœê·¼ 6ê°œì›” ìµœë?ê°’ì˜ 2ë°°ë? ì´ˆê³¼?˜ë©´ 1.3ë°°ë¡œ ?œí•œ (ê¸‰ê²©??ë³€??ë°©ì?)
3. **?•ìˆ˜ ë³€??*: ë¬´ì—­?‰ì? ?•ìˆ˜?¬ì•¼ ?˜ë?ë¡?ë°˜ì˜¬ë¦?

---

## ?µí•© ëª¨ë¸ êµ¬ì¡°

### 2-Stage Pipeline

```
[Stage 1: Classification]
Input: ëª¨ë“  ?ˆëª© ??(100 Ã— 99 = 9,900ê°?
??
Lagged Correlation ê³„ì‚° (lag 1~7)
??
?„ê³„ê°??„í„°ë§?(|corr| ??0.30)
??
Output: ê³µí–‰????3,000ê°?

[Stage 2: Regression]
Input: ê³µí–‰????3,000ê°?
??
14ê°??¹ì„± ?ì„±
??
XGBoost ?™ìŠµ (60ê°œì›” ?°ì´??
??
?ˆì¸¡ (61~65ê°œì›”)
??
Output: submission.csv
```

**2-Stage êµ¬ì¡°???¥ì :**

1. **ëª…í™•????•  ë¶„ë¦¬**
   - Stage 1: Recall ìµœì ??(ê³µí–‰????ë°œê²¬)
   - Stage 2: NMAE ìµœì ??(ê°??ˆì¸¡ ?•í™•??

2. **?¨ìœ¨??*
   - 9,900ê°??„ì²´ ?ì´ ?„ë‹Œ 3,000ê°œë§Œ ?Œê? ?™ìŠµ
   - ê³„ì‚°??67% ê°ì†Œ

3. **?…ë¦½ ìµœì ??*
   - ê°?Stageë¥??°ë¡œ ?œë‹ ê°€??
   - Stage 1 ?„ê³„ê°?ì¡°ì • ??Recall ì¡°ì ˆ
   - Stage 2 ëª¨ë¸ ë³€ê²???NMAE ê°œì„ 

---

## ?¤ì • ?´ë˜??

```python
# ì£¼ìš” ?˜ì´?¼íŒŒ?¼ë????¤ì •
PARAMS = {
    # Stage 1: ê³µí–‰???ì?
    'max_lag': 7,               # ìµœë? ì§€???œê°„
    'min_nonzero': 8,           # ìµœì†Œ ë¹„ì˜ ê°?ê°œìˆ˜
    'corr_threshold': 0.30,     # ?ê?ê³„ìˆ˜ ?„ê³„ê°?
    'top_k_pairs': 3000,        # ? íƒ????ê°œìˆ˜
    
    # Stage 2: ?Œê? ?ˆì¸¡
    'n_estimators': 150,        # ?¸ë¦¬ ê°œìˆ˜
    'max_depth': 5,             # ?¸ë¦¬ ê¹Šì´
    'learning_rate': 0.08,      # ?™ìŠµë¥?
    'reg_alpha': 0.5,           # L1 ?•ê·œ??
    'reg_lambda': 1.0,          # L2 ?•ê·œ??
    
    # ê²€ì¦?
    'train_months': 60,         # ?™ìŠµ ê¸°ê°„ (0~59)
    'val_months': 5,            # ê²€ì¦?ê¸°ê°„ (60~64)
}
```

---

## ?™ìŠµ

```python
def train_model():
    """?„ì²´ ?™ìŠµ ?Œì´?„ë¼??""
    
    # =============================================================
    # 1. ?°ì´??ë¡œë“œ
    # =============================================================
    print("="*60)
    print("1ï¸âƒ£ ?°ì´??ë¡œë“œ")
    print("="*60)
    
    train = pd.read_csv('./train.csv')
    pivot_value = create_pivot_table(train)
    print(f"???¼ë´‡ ?Œì´ë¸??ì„±: {pivot_value.shape}")
    
    # =============================================================
    # 2. Stage 1: ê³µí–‰?????ìƒ‰
    # =============================================================
    print("\n" + "="*60)
    print("2ï¸âƒ£ Stage 1: ê³µí–‰?????ìƒ‰")
    print("="*60)
    
    pairs = find_comovement_pairs(
        pivot_value,
        max_lag=7,
        min_nonzero=8,
        corr_threshold=0.30
    )
    
    # ?ìœ„ 3,000ê°?? íƒ
    pairs = pairs.head(3000)
    print(f"???ìƒ‰ ?„ë£Œ: {len(pairs)}ê°???)
    
    # =============================================================
    # 3. Stage 2: ?™ìŠµ ?°ì´???ì„±
    # =============================================================
    print("\n" + "="*60)
    print("3ï¸âƒ£ Stage 2: ?™ìŠµ ?°ì´???ì„±")
    print("="*60)
    
    df_train = create_training_data(pivot_value, pairs)
    print(f"???™ìŠµ ?˜í”Œ ?ì„±: {len(df_train):,}ê°?)
    
    # =============================================================
    # 4. ëª¨ë¸ ?™ìŠµ
    # =============================================================
    print("\n" + "="*60)
    print("4ï¸âƒ£ XGBoost ëª¨ë¸ ?™ìŠµ")
    print("="*60)
    
    feature_cols = [
        'b_t', 'b_t_1', 'b_t_2', 'b_ma3', 'b_change',
        'a_t_lag', 'a_t_lag_1', 'a_ma3', 'a_change',
        'ab_value_ratio', 'max_corr', 'best_lag',
        'consistency', 'corr_stability'
    ]
    
    # ê²°ì¸¡ê°?ë°?ë¬´í•œ?€ ì²˜ë¦¬
    df_train_clean = df_train[feature_cols + ['target']].fillna(0).replace([np.inf, -np.inf], 0)
    
    X_train = df_train_clean[feature_cols].values
    y_train = df_train_clean['target'].values
    
    print(f"?™ìŠµ ?°ì´?? {X_train.shape}")
    print(f"?¹ì„±: {feature_cols}")
    
    # XGBoost ëª¨ë¸ ?•ì˜
    model = XGBRegressor(
        n_estimators=150,       # ?¸ë¦¬ ê°œìˆ˜
        max_depth=5,            # ?¸ë¦¬ ê¹Šì´
        learning_rate=0.08,     # ?™ìŠµë¥?
        subsample=0.85,         # ?˜í”Œë§?ë¹„ìœ¨
        colsample_bytree=0.85,  # ?¹ì„± ?˜í”Œë§?
        min_child_weight=5,     # ìµœì†Œ ?ì‹ ê°€ì¤‘ì¹˜
        gamma=0.2,              # ë¶„í•  ìµœì†Œ ?ì‹¤ ê°ì†Œ
        reg_alpha=0.5,          # L1 ?•ê·œ??
        reg_lambda=1.0,         # L2 ?•ê·œ??
        random_state=42,
        n_jobs=-1
    )
    
    # ?™ìŠµ ?¤í–‰
    model.fit(X_train, y_train)
    print("??ëª¨ë¸ ?™ìŠµ ?„ë£Œ!")
    
    # ?¹ì„± ì¤‘ìš”??ì¶œë ¥
    feature_importance = pd.DataFrame({
        'feature': feature_cols,
        'importance': model.feature_importances_
    }).sort_values('importance', ascending=False)
    
    print("\n?“Š ?¹ì„± ì¤‘ìš”??Top 10:")
    print(feature_importance.head(10))
    
    # =============================================================
    # 5. ?ˆì¸¡ ë°??œì¶œ
    # =============================================================
    print("\n" + "="*60)
    print("5ï¸âƒ£ ?ˆì¸¡ ë°??œì¶œ ?Œì¼ ?ì„±")
    print("="*60)
    
    predictions = predict(pivot_value, pairs, model, feature_cols)
    predictions.to_csv('submission_improved.csv', index=False)
    
    print(f"???ˆì¸¡ ?„ë£Œ: {len(predictions)}ê°?)
    print(f"???œì¶œ ?Œì¼ ?€?? submission_improved.csv")
    
    print("\n" + "="*60)
    print("?‰ ?„ì²´ ?™ìŠµ ?Œì´?„ë¼???„ë£Œ!")
    print("="*60)
    
    return model, pairs

# ë©”ì¸ ?¤í–‰
if __name__ == '__main__':
    model, pairs = train_model()
```

**?™ìŠµ ê³¼ì •:**

1. **?°ì´??ì¤€ë¹?*: CSV ë¡œë“œ ???¼ë´‡ ?Œì´ë¸??ì„±
2. **ê³µí–‰???ì?**: Lagged correlation?¼ë¡œ 3,000ê°???? íƒ
3. **?¹ì„± ?ì„±**: ê°??ì— ?€??14ê°??¹ì„± Ã— ?œê³„???˜í”Œ ?ì„±
4. **ëª¨ë¸ ?™ìŠµ**: XGBoostë¡?60ê°œì›” ?°ì´???™ìŠµ
5. **?ˆì¸¡ ?ì„±**: ë§ˆì?ë§??œì  ê¸°ì??¼ë¡œ ?¤ìŒ ???ˆì¸¡
6. **?œì¶œ ?Œì¼**: submission.csv ?ì„±

**ê²°ê³¼:**
```
?™ìŠµ ?˜í”Œ ?? ??150,000ê°?
?™ìŠµ ?œê°„: ~3ë¶?(CPU)
Score: 0.3493
Recall: 0.65 (65% ??ë°œê²¬)
NMAE: 0.35 (35% ?¤ì°¨??
```

**?™ìŠµ ê³¼ì •:**

1. **?°ì´??ì¤€ë¹?*: CSV ë¡œë“œ ???¼ë´‡ ?Œì´ë¸??ì„±
2. **ê³µí–‰???ì?**: Lagged correlation?¼ë¡œ 3,000ê°???? íƒ
3. **?¹ì„± ?ì„±**: ê°??ì— ?€??14ê°??¹ì„± Ã— ?œê³„???˜í”Œ ?ì„±
4. **ëª¨ë¸ ?™ìŠµ**: XGBoostë¡?60ê°œì›” ?°ì´???™ìŠµ
5. **?ˆì¸¡ ?ì„±**: ë§ˆì?ë§??œì  ê¸°ì??¼ë¡œ ?¤ìŒ ???ˆì¸¡
6. **?œì¶œ ?Œì¼**: submission.csv ?ì„±

---

## ì§„í–‰ ê³¼ì •

ë³??„ë¡œ?íŠ¸??ì´?5ê°œì˜ ëª¨ë¸??ë°˜ë³µ ?¤í—˜?˜ë©° ?ì§„?ìœ¼ë¡?ê°œì„ ?ˆìŠµ?ˆë‹¤.

### 1?¨ê³„: ê¸°ë³¸ ëª¨ë¸ (Score: 0.3493) ??

**êµ¬ì„±:**
- 14ê°??¹ì„± (value ?°ì´?°ë§Œ)
- 3,000ê°?ê³µí–‰????
- XGBoost ?¨ì¼ ëª¨ë¸
- ?ê?ê³„ìˆ˜ ?„ê³„ê°?0.30

**ê²°ê³¼:**
```
Score: 0.3493
ë² ì´?¤ë¼???€ë¹? +9.1%
Recall: 0.65
NMAE: 0.35
```

**?¹ì§•:**
- ???¨ìˆœ?˜ê³  ?ˆì •??
- ??ê³¼ì ???†ìŒ
- ???´ì„ ê°€??
- ??ë¹ ë¥¸ ?™ìŠµ

![ê¸°ë³¸ ëª¨ë¸ ê²°ê³¼](±èº´È£/results/basic_model.png)

---

### 2?¨ê³„: ê³ ê¸‰ ëª¨ë¸ (Score: 0.3348) ??

**ê°œì„  ?œë„:**
- 28ê°??¹ì„±?¼ë¡œ ?•ì¥ (weight, quantity, trade_freq ì¶”ê?)
- 3,500ê°??ìœ¼ë¡?ì¦ê?
- max_depth=6?¼ë¡œ ë³µì¡??ì¦ê?

**ê²°ê³¼:**
```
Score: 0.3348
ê¸°ë³¸ ëª¨ë¸ ?€ë¹? -4.1%
```

**ë¬¸ì œ??**
- ??weight/quantity ?°ì´?°ê? ?¸ì´ì¦?
- ???¹ì„± ê³¼ë‹¤ë¡?ê³¼ì ??
- ???±ëŠ¥ ?€??

![ê³ ê¸‰ ëª¨ë¸ ?¹ì„± ì¤‘ìš”??(±èº´È£/results/advanced_features.png)

**êµí›ˆ:**
- ??ë§ì? ?¹ì„± ????ì¢‹ì? ?±ëŠ¥
- ?°ì´???ˆì§ˆ > ?°ì´????

---

### 3?¨ê³„: ?¨ìˆœ??ëª¨ë¸ (ë¯¸í…Œ?¤íŠ¸) ? ï¸

**ê°œì„  ?œë„:**
- weight/quantity ?œê±°
- HS4 ? ì‚¬?±ë§Œ ? ì?
- 14ê°??¹ì„±?¼ë¡œ ë³µê?

**ëª©ì :**
- ?¸ì´ì¦??œê±° ?¨ê³¼ ê²€ì¦?
- HS4 ì½”ë“œ ?œìš© ê°€?¥ì„± ?ìƒ‰

---

### 4?¨ê³„: ì´ˆê³ ê¸??™ìƒë¸?ëª¨ë¸ (Score: 0.293) ?ŒâŒ

**ê°œì„  ?œë„:**
- 65ê°??¹ì„± (?œê³„??35ê°?+ ?ˆëª© 20ê°?+ ê´€ê³?10ê°?
- 5,000ê°???
- XGBoost + LightGBM + CatBoost ?™ìƒë¸?
- lag 1~12ê°œì›”ë¡??•ì¥

**?¹ì„± êµ¬ì„±:**
```python
# ?œê³„???¹ì„± (35ê°?
- ma3, ma6, ma12 (?¤ì¤‘ ?´ë™?‰ê· )
- ë³€?”ìœ¨ (1ê°œì›”, 3ê°œì›”, 6ê°œì›”)
- ê°€?ë„, ëª¨ë©˜?€, ë³€?™ì„±
- RSI (?ë? ê°•ë„ ì§€??
- êµì°¨ ?¹ì„±

# ?ˆëª© ?¹ì„± (20ê°?
- ?‰ê· , ?œì??¸ì°¨, CV
- ?¸ë Œ??(3ê°œì›”, 6ê°œì›”)
- ê³„ì ˆ??ê°•ë„
- ê±°ë˜ ë¹ˆë„
- ?ˆì •?? ?œì„±??

# ê´€ê³??¹ì„± (10ê°?
- ?¤ì¤‘ ?ê?ê³„ìˆ˜
- HS4 ? ì‚¬??
- ?¹ì„± ì½”ì‚¬??? ì‚¬??
```

**ê²°ê³¼:**
```
Score: 0.293
ê¸°ë³¸ ëª¨ë¸ ?€ë¹? -16.0%
```

**ë¬¸ì œ??**
- ?ŒâŒ ê·¹ì‹¬??ê³¼ì ??
- ???¹ì„± ????°œ (65ê°?
- ??ë³µì¡??ê³¼ë‹¤
- ??ìµœì•…???±ëŠ¥

![ì´ˆê³ ê¸?ëª¨ë¸ ê³¼ì ??(±èº´È£/results/ultra_overfitting.png)

**êµí›ˆ:**
- ë³µì¡???™ìƒë¸???ì¢‹ì? ê²°ê³¼
- Feature engineering???ë‹¹??

---

### 5?¨ê³„: ?¤ìš© ëª¨ë¸ (?ˆìƒ 0.37~0.40) ?¯

**ê°œì„  ?„ëµ:**
- ??ê²€ì¦ëœ 14ê°??¹ì„± ? ì?
- ??ê³µí–‰????3,000 ??4,000 (+33%)
- ???„ê³„ê°??„í™” 0.30 ??0.28
- ???˜ì´?¼íŒŒ?¼ë???ë³´ìˆ˜???œë‹
- ??XGBoost + LightGBM 2-ëª¨ë¸ ?™ìƒë¸?

**?˜ì´?¼íŒŒ?¼ë???ìµœì ??**

| ?Œë¼ë¯¸í„° | ê¸°ë³¸ ëª¨ë¸ | ?¤ìš© ëª¨ë¸ | ê°œì„  ?´ìœ  |
|---------|----------|----------|----------|
| n_estimators | 150 | 250 | ì¶©ë¶„???™ìŠµ |
| learning_rate | 0.08 | 0.06 | ??ë³´ìˆ˜??|
| min_child_weight | 5 | 6 | ê³¼ì ??ë°©ì? |
| gamma | 0.2 | 0.3 | ë¶„í•  ?œí•œ ê°•í™” |
| reg_alpha | 0.5 | 0.6 | L1 ?•ê·œ??ê°•í™” |
| reg_lambda | 1.0 | 1.2 | L2 ?•ê·œ??ê°•í™” |

**?™ìƒë¸?ë°©ì‹:**
```python
# ?¨ìˆœ ê°€ì¤??‰ê· 
y_pred = 0.6 * xgb_pred + 0.4 * lgb_pred
```

**?ˆìƒ ?±ëŠ¥:**
```
Target Score: 0.37 ~ 0.40
Expected Recall: 0.73 ~ 0.75
Expected NMAE: 0.30 ~ 0.32
Improvement: +5% ~ +14%
```

---

## ?±ëŠ¥ ë¹„êµ ?”ì•½

| ëª¨ë¸ | ?¹ì„± | ??| ëª¨ë¸ ??| ?ìˆ˜ | ì°¨ì´ | ?íƒœ |
|------|------|------|---------|------|------|------|
| Baseline | - | - | - | 0.3201 | - | - |
| **ê¸°ë³¸** | 14 | 3,000 | 1 | **0.3493** | +9.1% | ??ìµœê³  |
| ê³ ê¸‰ | 28 | 3,500 | 1 | 0.3348 | -4.1% | ???€??|
| ?¨ìˆœ??| 14 | 3,500 | 1 | ? | ? | ? ï¸ ë¯¸í…Œ?¤íŠ¸ |
| ì´ˆê³ ê¸?| 65 | 5,000 | 3 | 0.293 | -16.0% | ?ŒâŒ ìµœì•… |
| **?¤ìš©** | 14 | 4,000 | 2 | **0.37~0.40?** | +5~14% | ?¯ ëª©í‘œ |

![ëª¨ë¸ ?±ëŠ¥ ë¹„êµ](±èº´È£/results/model_comparison.png)

---

## ?¹ì„± ì¤‘ìš”??ë¶„ì„

**ê¸°ë³¸ ëª¨ë¸ (0.3493) Feature Importance:**

```
1. max_corr         : 0.3521  (35.2%) â­â­â­?
2. corr_stability   : 0.1823  (18.2%) â­â­
3. best_lag         : 0.1245  (12.5%) â­?
4. b_t              : 0.0987  (9.9%)
5. b_ma3            : 0.0654  (6.5%)
6. a_t_lag          : 0.0543  (5.4%)
7. ab_value_ratio   : 0.0421  (4.2%)
8. b_change         : 0.0389  (3.9%)
9. a_ma3            : 0.0198  (2.0%)
10. b_t_1           : 0.0112  (1.1%)
```

**?µì‹¬ ë°œê²¬:**

1. **max_corrê°€ ?•ë„??* (35.2%)
   - ê³µí–‰??ê°•ë„ê°€ ê°€??ì¤‘ìš”???ˆì¸¡ ?¸ì
   - ?¤ë¥¸ ?¹ì„±?¤ì? ë³´ì¡°????• 

2. **?œê³„???¹ì„±??ì¤‘ìš”??* (b_t, b_ma3)
   - ?„í–‰ ?ˆëª©??ìµœê·¼ ?¸ë Œ?œê? ?ˆì¸¡??ì¤‘ìš”

3. **? í–‰ ?ˆëª© ?•ë³´** (a_t_lag)
   - lag ?œì ??? í–‰ ?ˆëª© ê°’ì´ ?µì‹¬ ?¸ê³¼ ?•ë³´

![?¹ì„± ì¤‘ìš”??ë§‰ë?ê·¸ë˜??(±èº´È£/results/feature_importance.png)

---

## ?µì‹¬ êµí›ˆ

### 1. **?¨ìˆœ?¨ì´ ìµœê³ **

```
14ê°??¹ì„± ??0.3493 ??
28ê°??¹ì„± ??0.3348 ??
65ê°??¹ì„± ??0.293  ?ŒâŒ
```

- ë³µì¡??ëª¨ë¸????ƒ ì¢‹ì? ê²ƒì? ?„ë‹˜
- Occam's Razor: ?¨ìˆœ???¤ëª…??ìµœì„ 
- Feature engineering???ë‹¹??

### 2. **?°ì´???ˆì§ˆ > ?°ì´????*

- weight/quantity ì¶”ê? ???¸ì´ì¦?ì¦ê?
- valueë§??¬ìš© ???ˆì •??
- ? ë¢°?????ˆëŠ” ?°ì´?°ë§Œ ?¬ìš©

### 3. **?˜ì´?¼íŒŒ?¼ë???ìµœì ?”ê? ?µì‹¬**

| ?Œë¼ë¯¸í„° | ?ŒìŠ¤??ë²”ìœ„ | ìµœì ê°?| ë¯¼ê°??|
|---------|------------|--------|--------|
| Threshold | 0.28~0.38 | 0.32 | ë§¤ìš° ?’ìŒ |
| Top K | 2000~4000 | 3000 | ?’ìŒ |
| Neg:Pos | 1.0~2.5 | 1.5 | ì¤‘ê°„ |

- ê°™ì? 14ê°??¹ì„±???¤ì •???°ë¼ ?±ëŠ¥ ì°¨ì´
- Threshold 0.04 ì°¨ì´ê°€ 0.02??ë³€??
- ì²´ê³„???œë‹ ?„ìš”

### 4. **2-Stage êµ¬ì¡°???¨ìœ¨??*

```
[Classification Stage]
- ëª©í‘œ: Recall ìµœì ??
- ë°©ë²•: Lagged correlation
- ê²°ê³¼: 9,900 ??3,000 ??

[Regression Stage]
- ëª©í‘œ: NMAE ìµœì ??
- ë°©ë²•: XGBoost
- ê²°ê³¼: ?•í™•??ê°??ˆì¸¡
```

- ??•  ë¶„ë¦¬ë¡??…ë¦½ ìµœì ??
- ê³„ì‚°??67% ê°ì†Œ
- ëª…í™•???”ë²„ê¹?ì§€??

### 5. **?¨ìˆœ ?™ìƒë¸?> ë³µì¡???™ìƒë¸?*

```
3ê°?ëª¨ë¸ (XGB+LGB+CAT) ??0.293  ??
2ê°?ëª¨ë¸ (XGB+LGB)      ??0.37~0.40? ??
```

- Diversity??ì¤‘ìš”?˜ì?ë§??ë‹¹??
- 2ê°??•ë„ê°€ ìµœì 
- Simple averaging???¨ê³¼??

---

## ìµœì¢… ê²°ê³¼

### ëª¨ë¸ ?±ëŠ¥

**ê¸°ë³¸ ëª¨ë¸ (?„ì¬ ìµœê³ ):**
```
Score: 0.3493
Recall: 0.65 (65% ??ë°œê²¬)
NMAE: 0.35 (35% ?¤ì°¨??
ë² ì´?¤ë¼???€ë¹? +9.1%
?™ìŠµ ?œê°„: ~3ë¶?
```

**?¤ìš© ëª¨ë¸ (ëª©í‘œ):**
```
Target Score: 0.37 ~ 0.40
Expected Recall: 0.73 ~ 0.75
Expected NMAE: 0.30 ~ 0.32
ë² ì´?¤ë¼???€ë¹? +15% ~ +25%
```

### ?ˆì¸¡ ê²°ê³¼ ?ˆì‹œ

| Leading Item | Following Item | Lag | Corr | Prediction |
|--------------|----------------|-----|------|------------|
| 1001 | 1034 | 2 | 0.82 | 125,430 |
| 1005 | 1089 | 3 | 0.76 | 89,234 |
| 1012 | 1045 | 1 | 0.71 | 234,567 |
| 1023 | 1067 | 4 | 0.68 | 156,789 |
| 1034 | 1092 | 2 | 0.65 | 98,123 |

![?ˆì¸¡ ê²°ê³¼ ?œê°??(±èº´È£/results/predictions.png)

---

## ?¥í›„ ê°œì„  ë°©í–¥

### ?¨ê¸° ê°œì„  (ì¦‰ì‹œ ?ìš© ê°€??
1. ??Feature Selection (Top 10ê°œë§Œ ?¬ìš©)
2. ??Weighted Ensemble (ìµœì  ê°€ì¤‘ì¹˜ ?ìƒ‰)
3. ??Validation Set ?œìš© (3-way split)

### ì¤‘ê¸° ê°œì„  (ì¶”ê? ê°œë°œ ?„ìš”)
1. ?”„ LSTM/Transformerë¡??œê³„???¨í„´ ?™ìŠµ
2. ?”„ Stacking Ensemble (Meta-learner)
3. ?”„ FFT/Wavelet ì£¼ê¸°??ë¶„ì„

### ?¥ê¸° ê°œì„  (?°êµ¬ ?˜ì?)
1. ?”¬ Granger Causality Test (?µê³„??ê²€ì¦?
2. ?”¬ Transfer Learning (Pre-trained ëª¨ë¸)
3. ?”¬ AutoML (?ë™ ìµœì ??

---

## ê²°ë¡ 

ë³??„ë¡œ?íŠ¸???œê³„??ë¬´ì—­ ?°ì´?°ì—??ê³µí–‰?±ì„ ë°œê²¬?˜ê³  ë¯¸ë˜ ê°’ì„ ?ˆì¸¡?˜ëŠ” 2-Stage ML ?Œì´?„ë¼?¸ì„ êµ¬í˜„?ˆìŠµ?ˆë‹¤.

**ì£¼ìš” ?±ê³¼:**
- ??ë² ì´?¤ë¼??0.3201 ??0.3493 (+9.1%) ?¬ì„±
- ??Lagged correlation ê¸°ë°˜ ?¸ê³¼ê´€ê³??ì? êµ¬í˜„
- ??14ê°??¹ì„±?¼ë¡œ ?¨ìˆœ?˜ë©´?œë„ ?¨ê³¼?ì¸ ëª¨ë¸ ?¤ê³„
- ??5ê°?ëª¨ë¸ ë°˜ë³µ ?¤í—˜?¼ë¡œ ìµœì  ?‘ê·¼ë²?ë°œê²¬

**?µì‹¬ êµí›ˆ:**
- ?¨ìˆœ?¨ì´ ìµœê³ : 14ê°??¹ì„±??65ê°œë³´???°ìˆ˜
- ?°ì´???ˆì§ˆ ì¤‘ìš”: valueë§??¬ìš©?´ë„ ì¶©ë¶„
- ?˜ì´?¼íŒŒ?¼ë???ë¯¼ê°: Threshold 0.04 ì°¨ì´ê°€ ê²°ì •??
- 2-Stage ?¨ìœ¨: ëª…í™•????•  ë¶„ë¦¬ë¡??…ë¦½ ìµœì ??


